{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a688702",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix\n",
    "from misc.helpers import find_model_weights, calculate_label_distributions, prev_img, prev_img_gray, trans_norm2tensor, find_csv_files, get_image_paths_from_csv, get_image_paths_from_dir, get_image_filenames_by_label, create_vit_model\n",
    "from misc.visualization import *\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from model.vit_for_small_dataset import ViT\n",
    "from utils.imageQualityDataset import ImageQualityDataset\n",
    "from utils.imageAttentionGlobalAvgDataset import ImageAttentionGlobalAvgDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b2b95",
   "metadata": {},
   "source": [
    "# 1. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=256\n",
    "patch_size=16\n",
    "num_classes=5\n",
    "depth = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_model()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization parameters (mean and std)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the transformation including normalization\n",
    "img2tensor_normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ab1f2",
   "metadata": {},
   "source": [
    "# Beginn Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7f8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "class TransformedImageDataset(Dataset):\n",
    "    def __init__(self, images, transform_funcs, crop_size=(50, 50), crop_position=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - images (list): List of image file paths or PIL images.\n",
    "        - transform_funcs (list): list of Transformation functions (e.g., apply_compression_patch, apply_blur_patch).\n",
    "        - crop_size (tuple): Size of the cropped area.\n",
    "        - crop_position (str): List of crop positions for transformation (e.g., 'center', 'top-left', etc.).\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.transform_funcs = transform_funcs\n",
    "        self.crop_size = crop_size\n",
    "        self.crop_position = crop_position or None\n",
    "        \n",
    "        # Calculate total transformations per image\n",
    "        self.transforms_per_image = []\n",
    "        for transform_func in transform_funcs:\n",
    "            self.transforms_per_image.append((transform_func, self.crop_position))\n",
    "        \n",
    "        print(f\"crop positions: {self.crop_position}\")\n",
    "        print(f\"Total transformations per image: {len(self.transforms_per_image)}\")\n",
    "        print(f\"Total images: {len(self.images)}\")\n",
    "\n",
    "        # Define the transformation including normalization\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.images[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\") if isinstance(img_path, str) else img_path\n",
    "        # Preprocess the image\n",
    "        img = self.preprocess(img)\n",
    "        org_img = img.copy()\n",
    "        # Apply all the transformations with the specified crop position and level parameters\n",
    "        transformed_imgs = []\n",
    "        for transform_func, crop_pos in self.transforms_per_image:\n",
    "            transformed_img = transform_func(img, crop_size=self.crop_size, crop_pos=crop_pos)\n",
    "            transformed_imgs.append(transformed_img)\n",
    "        \n",
    "        # Return the original image and an array of the transformed images\n",
    "        return org_img, transformed_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "import random\n",
    "from io import BytesIO\n",
    "from functools import partial  # Import partial to handle function parameters\n",
    "\n",
    "\n",
    "csv_file = 'assets/Test/Obs0.csv'\n",
    "\n",
    "img_directory_path = 'assets/Test/DSX'\n",
    "\n",
    "# Get the list of image paths from the CSV file\n",
    "# Initialize a list to store the image paths\n",
    "image_paths = []\n",
    "\n",
    "# Read the CSV file and extract the image filenames\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        image_filename = row[0]\n",
    "        vote = row[1]\n",
    "        # fitler only images with vote of 5\n",
    "        if int(vote) == 5:\n",
    "            image_path = os.path.join(img_directory_path, image_filename)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "            \n",
    "# Transformationen und Levels definieren\n",
    "transform_funcs = [\n",
    "    transformations.apply_occlusion_patch,\n",
    "    partial(transformations.apply_compression_patch, quality=0),  # Use `partial` to set parameters\n",
    "    transformations.apply_grayscale_patch,\n",
    "    partial(transformations.apply_blur_patch, blur_radius=6)  # Use `partial` to set parameters\n",
    "]\n",
    "\n",
    "# Manually assign names to partial functions for use in plotting\n",
    "for func in transform_funcs:\n",
    "    if isinstance(func, partial):\n",
    "        func.__name__ = f\"{func.func.__name__}\"  # Set a name for partial function\n",
    "\n",
    "\n",
    "crop_position = \"center\"\n",
    "# get transformation function name from the function name\n",
    "crop_size = (96,96)\n",
    "\n",
    "# Dataset initialisieren\n",
    "dataset = TransformedImageDataset(\n",
    "    # images=image_paths[:30], # images for testing\n",
    "    images=image_paths, # images for testing\n",
    "    transform_funcs=transform_funcs,\n",
    "    crop_size=crop_size,\n",
    "    crop_position=crop_position\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example single image\n",
    "seed = random.randint(0, len(dataset)-1)\n",
    "org_img, _ = dataset[seed]\n",
    "\n",
    "plt.imshow(org_img)\n",
    "plt.title(f\"Orginal Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Show all levels in 1 row\n",
    "fig, axes = plt.subplots(1, len(transform_funcs) + 1, figsize=(15, 5))\n",
    "\n",
    "# Get the original and transformed images\n",
    "org_img, trans_imgs = dataset[seed]\n",
    "\n",
    "# Display the original image with a title\n",
    "axes[0].imshow(org_img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "for ax in axes:\n",
    "     ax.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "# Loop over the transformed images and display each with the function name as the title\n",
    "for i, (img, (transform_func, _)) in enumerate(zip(trans_imgs, dataset.transforms_per_image)):\n",
    "    # Get the name of the transformation function\n",
    "    transform_name = transform_func.__name__.split(\"_\")[1]\n",
    "    axes[i + 1].imshow(img)\n",
    "    axes[i + 1].set_title(transform_name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba594a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL\"\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1,6)]\n",
    "weight_file = weight_files[0]\n",
    "print(weight_file)\n",
    "\n",
    "output_dir = f\"results/Attention_maps/compare_transformations/{crop_position}\"\n",
    "# create output_dir if not exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "layer_idx = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(output_dir, \"test_file.txt\")\n",
    "try:\n",
    "    with open(test_path, \"w\") as test_file:\n",
    "        test_file.write(\"Testing write permissions.\\n\")\n",
    "    print(test_path)\n",
    "    os.remove(test_path)  # Clean up\n",
    "    print(f\"Write permissions are granted in directory: {output_dir}.\")\n",
    "except PermissionError as e:\n",
    "    print(f\"Permission error in directory: {output_dir}. {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_average_attention_in_crop(attention_map, crop_bbox, patch_size):\n",
    "    \"\"\"\n",
    "    Calculate the average attention within a specified crop region.\n",
    "\n",
    "    Args:\n",
    "    - attention_map (torch.Tensor or np.ndarray): Attention map of shape (H, W).\n",
    "    - crop_bbox (tuple): Bounding box coordinates (left, top, right, bottom) in pixel space.\n",
    "    - patch_size (int): Size of each patch in pixels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Average attention within the cropped region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the bounding box from pixel space to patch space\n",
    "    left, top, right, bottom = [coord // patch_size for coord in crop_bbox]\n",
    "    print(\"Left, Top, Right, Bottom\", left, top, right, bottom)\n",
    "    # print(left, top, right, bottom)\n",
    "    # Ensure that the attention_map is in numpy format for easier indexing\n",
    "    if isinstance(attention_map, torch.Tensor):\n",
    "        attention_map = attention_map.cpu().numpy()\n",
    "\n",
    "    # Extract the attention map within the crop region\n",
    "    crop_attention = attention_map[top:bottom, left:right]\n",
    "\n",
    "    # Calculate the average attention in the cropped region\n",
    "    avg_attention = np.mean(crop_attention)\n",
    "    \n",
    "    return avg_attention\n",
    "\n",
    "def calculate_sum_attention_in_crop(attention_map, crop_bbox, patch_size):\n",
    "    \"\"\"\n",
    "    Calculate the sum of attention within a specified crop region.\n",
    "\n",
    "    Args:\n",
    "    - attention_map (torch.Tensor or np.ndarray): Attention map of shape (H, W).\n",
    "    - crop_bbox (tuple): Bounding box coordinates (left, top, right, bottom) in pixel space.\n",
    "    - patch_size (int): Size of each patch in pixels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Sum of attention within the cropped region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the bounding box from pixel space to patch space\n",
    "    left, top, right, bottom = [coord for coord in crop_bbox]\n",
    "    print(\"Left, Top, Right, Bottom\", left, top, right, bottom)\n",
    "    # print(left, top, right, bottom)\n",
    "\n",
    "    # Ensure that the attention_map is in numpy format for easier indexing\n",
    "    if isinstance(attention_map, torch.Tensor):\n",
    "        attention_map = attention_map.cpu().numpy()\n",
    "\n",
    "    # Extract the attention map within the crop region\n",
    "    crop_attention = attention_map[top:bottom, left:right]\n",
    "    # print(f\"Crop Atenntion shape: {crop_attention.shape}\")\n",
    "\n",
    "    # print(f\"Crop Attention: {crop_attention}\")\n",
    "    # print(f\"Crop Attention Sum: {crop_attention.sum()}\")\n",
    "    # print(f\"Crop Attention Mean: {crop_attention.mean()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the sum of attention in the cropped region\n",
    "    sum_attention = np.sum(crop_attention)\n",
    "\n",
    "    return sum_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img_gray(img: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Process the image for previewing purposes with grayscale and contrast adjustment.\n",
    "\n",
    "    This function applies transformations to resize and center crop the input image to a specified size.\n",
    "    It then converts the image to grayscale and enhances the contrast by scaling the luminance values.\n",
    "    \n",
    "    Parameters:\n",
    "    img (PIL.Image): The input image.\n",
    "\n",
    "    Returns:\n",
    "    img_bw_contrast_rgb (PIL.Image): The processed image with enhanced contrast in RGB format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    img_bw = img.convert('L')\n",
    "\n",
    "    # Convert grayscale image to YCbCr color space\n",
    "    img_ycbcr = img_bw.convert('YCbCr')\n",
    "    y, cb, cr = img_ycbcr.split()\n",
    "    # Convert Y channel to NumPy arrays\n",
    "    y_array = np.array(y)\n",
    "\n",
    "    # Perform contrast adjustment on the Y component\n",
    "    y_array = np.clip(y_array * 2.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Merge the adjusted Y, Cb, Cr components back into an image\n",
    "    img_bw_contrast = Image.merge('YCbCr', (Image.fromarray(y_array), cb, cr))\n",
    "\n",
    "    # Convert the image back to RGB for display\n",
    "    img_bw_contrast_rgb = img_bw_contrast.convert('RGB')\n",
    "    \n",
    "    return img_bw_contrast_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a16aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1938ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_data_avg = []\n",
    "attention_data_sum = []\n",
    "\n",
    "for image_idx in range(len(dataset)):\n",
    "\n",
    "    # Creating a subplot\n",
    "    fig, axes = plt.subplots(len(weight_files)+1, len(transform_funcs)+1, figsize=((len(transform_funcs)+1)*7, (len(weight_files)+1)*5))\n",
    "    # fig.suptitle(\"Attention Map Comparison Across Transformations and AIOs\", fontsize=fontsize+2)\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "\n",
    "    org_img, trans_imgs = dataset[image_idx]\n",
    "    org_img_pre = preprocess_img_gray(org_img)\n",
    "\n",
    "    # Displaying the original image and transformed images in the first row\n",
    "    axes[0, 0].imshow(org_img)\n",
    "    axes[0, 0].set_title(\"Original\", fontsize=fontsize)\n",
    "\n",
    "    for i, (img, (transform_func,_)) in enumerate(zip(trans_imgs, dataset.transforms_per_image)):\n",
    "        transform_name = transform_func.__name__.split(\"_\")[1]\n",
    "        # First Character of transform_name in Uppercase\n",
    "        transform_name = transform_name[0].upper() + transform_name[1:]\n",
    "        axes[0, i+1].imshow(img)\n",
    "        axes[0, i+1].set_title(transform_name, fontsize=fontsize)\n",
    "\n",
    "    # Displaying the attention maps for each weight_file in the next row\n",
    "    for i, weight_file in enumerate(weight_files):\n",
    "        model = create_vit_model(weights_path=weight_file)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        weight_name = os.path.basename(weight_file).split('.')[0]\n",
    "\n",
    "        # Calculate attention maps for the original and transformed images\n",
    "        img_pre_org = trans_norm2tensor(org_img, image_size, transformation_function=None)\n",
    "        _, attention_org = get_attention_maps(model, img_pre_org, patch_size, device)\n",
    "        # get attention in layer_idx layer\n",
    "        layer_attention_org = attention_org[layer_idx]\n",
    "        # normalize the mean head attention (mean of all heads, axis=0)\n",
    "        layer_mean_org = normalize_attention_maps(np.mean(layer_attention_org, axis=0))\n",
    "        print(f\"Layer Mean Org Shape: {layer_mean_org.shape}\")\n",
    "        # print(f\"Layer Mean Org: {layer_mean_org}\")\n",
    "        # print(f\"Layer Mean Org Sum: {layer_mean_org.sum()}\")\n",
    "        # print min max value of layer_mean_org\n",
    "        print(f\"Layer Mean Org Min: {layer_mean_org.min()}\")\n",
    "        print(f\"Layer Mean Org Max: {layer_mean_org.max()}\")\n",
    "\n",
    "\n",
    "\n",
    "        heatmap = sns.heatmap(layer_mean_org, cmap=\"jet\", alpha=0.7, ax=axes[i+1, 0])\n",
    "\n",
    "        # Adjust colorbar fontsize\n",
    "        cbar = heatmap.collections[0].colorbar  # Access the colorbar of the heatmap\n",
    "        cbar.ax.tick_params(labelsize=fontsize-2)  # Adjust the fontsize of the colorbar ticks\n",
    "        cbar.set_ticks([0, 0.25, 0.5, 0.75, 1])\n",
    "\n",
    "        axes[i+1, 0].imshow(org_img_pre)\n",
    "        axes[i+1, 0].set_title(f\"Attention Map {weight_name}\", fontsize=fontsize)\n",
    "\n",
    "        # Calculate attention maps for the transformed images\n",
    "        imgs_pre_trans = [trans_norm2tensor(img, image_size, transformation_function=None) for img in trans_imgs]\n",
    "        atts_trans = [get_attention_maps(model, img_pre_trans, patch_size, device)[1] for img_pre_trans in imgs_pre_trans]\n",
    "        layer_atts_trans = [att[layer_idx] for att in atts_trans]\n",
    "        layer_mean_trans = [normalize_attention_maps(np.mean(layer_att, axis=0)) for layer_att in layer_atts_trans]\n",
    "\n",
    "        crop_bbox = transformations.get_random_bbox(org_img, crop_size=crop_size, crop_pos=crop_position)\n",
    "        # print(f\"Crop BBox: {crop_bbox}\")\n",
    "\n",
    "        # Calculate the sum of attention within the cropped region\n",
    "        sum_attention_org = calculate_sum_attention_in_crop(\n",
    "            attention_map=layer_mean_org, \n",
    "            crop_bbox=crop_bbox, \n",
    "            patch_size=patch_size)\n",
    "        \n",
    "        # Store the data in the dictionary\n",
    "        attention_data_sum.append({\"Transformation\": f\"original\", \"Attention Sum\": sum_attention_org, \"Weight File\": weight_name})\n",
    "        # print(f\"Original Attention: {sum_attention_org}\")\n",
    "        # Store average attention for each transformation level and weight\n",
    "        for j, (lmt, img, (transform_func,_)) in enumerate(zip(layer_mean_trans, trans_imgs, dataset.transforms_per_image)):\n",
    "\n",
    "            # Calculate the sum of attention within the cropped region\n",
    "            sum_attention = calculate_sum_attention_in_crop(\n",
    "                attention_map=lmt, \n",
    "                crop_bbox=crop_bbox, \n",
    "                patch_size=patch_size)\n",
    "\n",
    "            # Store the data in the dictionary\n",
    "            attention_data_sum.append({\"Transformation\": f\"{transform_func.__name__.split('_')[1]}\", \"Attention Sum\": sum_attention, \"Weight File\": weight_name})\n",
    "            # print(f\"Transfromed Attention ({transform_func.__name__.split('_')[1]}): {sum_attention}\")\n",
    "\n",
    "            # Calculate difference in attention\n",
    "            diff_att = (lmt - layer_mean_org)\n",
    "            diff_att_round = np.round(diff_att, 2)\n",
    "\n",
    "            # Create the heatmap for the difference in attention\n",
    "            heatmap = sns.heatmap(diff_att_round, cmap=\"seismic\", alpha=0.7, ax=axes[i+1, j+1], vmin=-1, vmax=1)\n",
    "\n",
    "            # Adjust the colorbar fontsize\n",
    "            cbar = heatmap.collections[0].colorbar  # Access the colorbar of the heatmap\n",
    "            cbar.ax.tick_params(labelsize=fontsize-2)  # Set the fontsize for the colorbar ticks\n",
    "            cbar.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "            # Display the transformed image on top of the heatmap\n",
    "            axes[i+1, j+1].imshow(preprocess_img_gray(img))\n",
    "\n",
    "            # Format the transformation name (capitalize first letter)\n",
    "            transform_name = transform_func.__name__.split(\"_\")[1]\n",
    "            transform_name = transform_name[0].upper() + transform_name[1:]\n",
    "\n",
    "            # Set the title for the subplot with the desired fontsize\n",
    "            # axes[i+1, j+1].set_title(f\"{transform_name} - Attention Map {weight_name} (diff)\", fontsize=fontsize)\n",
    "            axes[i+1, j+1].set_title(f\"{transform_name} - Difference\", fontsize=fontsize)\n",
    "\n",
    "        \n",
    "        # Clear the model from memory\n",
    "        del model\n",
    "\n",
    "    # Adjust layout of subplots\n",
    "    plt.tight_layout()\n",
    "    # Define output path\n",
    "    output_path = os.path.join(output_dir, f\"attention_map_comparison_patch_transformations_img_{image_idx}_layer_{layer_idx}_.png\")\n",
    "    print(f\"Saving image to {output_path}\")\n",
    "    try:\n",
    "        print(f\"Attempting to save image to: {output_path}\")\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "    except PermissionError as e:\n",
    "        print(f\"Permission error while saving {output_path}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    # plt.savefig(output_path, dpi=300)\n",
    "    # plt.close(fig)\n",
    "\n",
    "# Create a DataFrame for boxplot\n",
    "attention_df_sum = pd.DataFrame(attention_data_sum)\n",
    "attention_df_avg = pd.DataFrame(attention_data_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d7946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Convert attention_data_avg to a DataFrame\n",
    "attention_df_avg = pd.DataFrame(attention_data_sum)\n",
    "\n",
    "# Define the order of the transformations\n",
    "transformations_order = [\"original\", \"occlusion\", \"compression\", \"grayscale\", \"blur\"]\n",
    "\n",
    "# Ensure the 'Transformation' column is of a category type with the defined order\n",
    "attention_df_avg['Transformation'] = pd.Categorical(attention_df_avg['Transformation'], categories=transformations_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by 'Transformation'\n",
    "attention_df_avg.sort_values('Transformation', inplace=True)\n",
    "\n",
    "# Create a pivot table for Average Attention\n",
    "pivot_table_avg = pd.pivot_table(\n",
    "    attention_df_avg,\n",
    "    values=\"Attention Sum\",    # Values to summarize\n",
    "    index=\"Transformation\",    # Rows\n",
    "    columns=\"Weight File\",     # Columns\n",
    "    aggfunc=\"mean\"             # Aggregation function\n",
    ")\n",
    "\n",
    "# Optionally format the table (e.g., rounding values for readability)\n",
    "pivot_table_avg = pivot_table_avg.round(3)\n",
    "\n",
    "# Save the pivot table as a CSV for inspection\n",
    "pivot_table_avg.to_csv(os.path.join(output_dir, \"average_attention_summary.csv\"))\n",
    "\n",
    "# Print the table to verify\n",
    "print(pivot_table_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Convert attention_data_avg to a DataFrame\n",
    "attention_df_avg = pd.DataFrame(attention_data_sum)\n",
    "\n",
    "# Define the order of the transformations\n",
    "transformations_order = [\"original\", \"occlusion\", \"compression\", \"grayscale\", \"blur\"]\n",
    "\n",
    "# Ensure the 'Transformation' column is of a category type with the defined order\n",
    "attention_df_avg['Transformation'] = pd.Categorical(attention_df_avg['Transformation'], categories=transformations_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by 'Transformation'\n",
    "attention_df_avg.sort_values('Transformation', inplace=True)\n",
    "\n",
    "# Create a pivot table for Average Attention\n",
    "pivot_table_avg = pd.pivot_table(\n",
    "    attention_df_avg,\n",
    "    values=\"Attention Sum\",    # Values to summarize\n",
    "    index=\"Transformation\",    # Rows\n",
    "    columns=\"Weight File\",     # Columns\n",
    "    aggfunc=\"mean\"             # Aggregation function  \n",
    "\n",
    ")\n",
    "\n",
    "# Round the original pivot table values to 2 decimal places\n",
    "pivot_table_avg = pivot_table_avg.round(2)\n",
    "# Save the original pivot table\n",
    "pivot_table_avg.to_csv(os.path.join(output_dir, \"average_attention_summary.csv\"))\n",
    "\n",
    "# Calculate percentage change relative to the \"original\" row\n",
    "original_values = pivot_table_avg.loc[\"original\"].round(2)\n",
    "\n",
    "# Calculate percentage change\n",
    "percentage_change_table = (pivot_table_avg - original_values) / original_values * 100\n",
    "\n",
    "# Format the table with original values and percentage changes\n",
    "formatted_table = pivot_table_avg.copy()\n",
    "\n",
    "# Iterate over transformations and modify the table\n",
    "for transformation in transformations_order:\n",
    "    if transformation == \"original\":\n",
    "        # Round the original values to 2 decimal places\n",
    "        formatted_table.loc[transformation] = formatted_table.loc[transformation].round(2)\n",
    "    else:\n",
    "        # Add percentage change values and append '%'\n",
    "        formatted_table.loc[transformation] = (\n",
    "            percentage_change_table.loc[transformation]\n",
    "            .round(2)  # Round percentage change\n",
    "            .astype(str)  # Convert to string\n",
    "            + \" %\"  # Append '%'\n",
    "        )\n",
    "\n",
    "# Save the formatted table to a CSV\n",
    "formatted_table.to_csv(os.path.join(output_dir, \"formatted_percentage_change_attention_summary.csv\"))\n",
    "\n",
    "# Print the formatted table for verification\n",
    "print(\"Formatted Table with Original Values and Percentage Changes:\")\n",
    "print(formatted_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f22d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directory to save the boxplots if it doesn't exist\n",
    "boxplot_dir = os.path.join(output_dir, \"boxplots\")\n",
    "os.makedirs(boxplot_dir, exist_ok=True)\n",
    "\n",
    "# Calculate the global y-axis limits for consistent scale\n",
    "y_min = attention_df_avg[\"Attention Sum\"].min() - 1000\n",
    "y_max = attention_df_avg[\"Attention Sum\"].max() + 1000\n",
    "\n",
    "# Iterate over each \"Weight File\" and create a boxplot for each\n",
    "for weight_file in attention_df_avg['Weight File'].unique():\n",
    "    # Filter the DataFrame for the current weight file\n",
    "    filtered_df = attention_df_avg[attention_df_avg['Weight File'] == weight_file]\n",
    "    \n",
    "    # Create a boxplot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(\n",
    "        data=filtered_df,\n",
    "        x=\"Transformation\",\n",
    "        y=\"Attention Sum\",\n",
    "        order=transformations_order,  # Ensures the correct order of the x-axis\n",
    "        palette=\"Set2\"\n",
    "    )\n",
    "    \n",
    "    # Set consistent y-axis limits\n",
    "    plt.ylim(y_min, y_max)\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title(f\"Attention Sum Distribution for {weight_file}\", fontsize=14)\n",
    "    plt.xlabel(\"Transformation\", fontsize=12)\n",
    "    plt.ylabel(\"Attention Sum\", fontsize=12)\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels for readability\n",
    "    \n",
    "    # Save the plot\n",
    "    boxplot_path = os.path.join(boxplot_dir, f\"boxplot_{weight_file.replace(' ', '_')}.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(boxplot_path)\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "\n",
    "    print(f\"Saved boxplot for {weight_file} at {boxplot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Convert attention_data_avg to a DataFrame\n",
    "attention_df_avg = pd.DataFrame(attention_data_sum)\n",
    "\n",
    "# Define the order of the transformations\n",
    "transformations_order = [\"original\", \"occlusion\", \"compression\", \"grayscale\", \"blur\"]\n",
    "\n",
    "# Ensure the 'Transformation' column is of a category type with the defined order\n",
    "attention_df_avg['Transformation'] = pd.Categorical(attention_df_avg['Transformation'], categories=transformations_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by 'Transformation'\n",
    "attention_df_avg.sort_values('Transformation', inplace=True)\n",
    "\n",
    "# Separate original and transformed rows\n",
    "original_attention = attention_df_avg[attention_df_avg['Transformation'] == 'original']\n",
    "transformed_attention = attention_df_avg[attention_df_avg['Transformation'] != 'original']\n",
    "\n",
    "# Function to compute mean ± std\n",
    "def mean_std_formatter(x):\n",
    "    mean = x.mean()\n",
    "    std = x.std()\n",
    "    return f\"{mean:.2f} (+/- {std:.2f})\"\n",
    "\n",
    "# Create a pivot table for transformed rows with mean ± std\n",
    "pivot_table_transformed = transformed_attention.pivot_table(\n",
    "    values=\"Attention Sum\",\n",
    "    index=\"Transformation\",\n",
    "    columns=\"Weight File\",\n",
    "    aggfunc=lambda x: mean_std_formatter(x)\n",
    ")\n",
    "\n",
    "# Create a pivot table for original rows (mean only, no std)\n",
    "pivot_table_original = original_attention.pivot_table(\n",
    "    values=\"Attention Sum\",\n",
    "    index=\"Transformation\",\n",
    "    columns=\"Weight File\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "# Combine the original and transformed pivot tables\n",
    "pivot_table_combined = pd.concat([pivot_table_original, pivot_table_transformed])\n",
    "\n",
    "# Save the pivot table as a CSV for inspection\n",
    "output_path = os.path.join(output_dir, \"attention_comparison_with_original.csv\")\n",
    "pivot_table_combined.to_csv(output_path)\n",
    "\n",
    "# Print the table to verify\n",
    "print(pivot_table_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dda05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Convert attention_data_avg to a DataFrame\n",
    "attention_df_avg = pd.DataFrame(attention_data_sum)\n",
    "\n",
    "# Define the order of the transformations\n",
    "transformations_order = [\"original\", \"occlusion\", \"compression\", \"grayscale\", \"blur\"]\n",
    "\n",
    "# Ensure the 'Transformation' column is of a category type with the defined order\n",
    "attention_df_avg['Transformation'] = pd.Categorical(attention_df_avg['Transformation'], categories=transformations_order, ordered=True)\n",
    "\n",
    "# Sort the DataFrame by 'Transformation'\n",
    "attention_df_avg.sort_values('Transformation', inplace=True)\n",
    "\n",
    "# Separate original and transformed rows\n",
    "original_attention = attention_df_avg[attention_df_avg['Transformation'] == 'original']\n",
    "transformed_attention = attention_df_avg[attention_df_avg['Transformation'] != 'original']\n",
    "\n",
    "# Function to compute mean ± std (% of mean)\n",
    "def mean_std_percentage_formatter(x):\n",
    "    mean = x.mean()\n",
    "    mean_percentage = \n",
    "\n",
    "    std = x.std()\n",
    "    std_percentage = (std / mean) * 100 if mean != 0 else 0  # Avoid division by zero\n",
    "    return f\"{mean:.2f} (+/- {std_percentage:.2f}%)\"\n",
    "\n",
    "# Create a pivot table for transformed rows with mean ± std as a percentage of the mean\n",
    "pivot_table_transformed = transformed_attention.pivot_table(\n",
    "    values=\"Attention Sum\",\n",
    "    index=\"Transformation\",\n",
    "    columns=\"Weight File\",\n",
    "    aggfunc=lambda x: mean_std_percentage_formatter(x)\n",
    ")\n",
    "\n",
    "# Create a pivot table for original rows (mean only, no std as there’s only one row per \"Weight File\")\n",
    "pivot_table_original = original_attention.pivot_table(\n",
    "    values=\"Attention Sum\",\n",
    "    index=\"Transformation\",\n",
    "    columns=\"Weight File\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "# Round the \"original\" values to 2 decimal places for consistency\n",
    "pivot_table_original = pivot_table_original.round(2)\n",
    "\n",
    "# Combine the original and transformed pivot tables\n",
    "pivot_table_combined = pd.concat([pivot_table_original, pivot_table_transformed])\n",
    "\n",
    "# Save the pivot table as a CSV for inspection\n",
    "output_path = os.path.join(output_dir, \"attention_comparison_with_std_percentage.csv\")\n",
    "pivot_table_combined.to_csv(output_path)\n",
    "\n",
    "# Print the table to verify\n",
    "print(\"Pivot Table with Mean ± Std as % of Mean:\")\n",
    "print(pivot_table_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e8190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table_avg, annot=True, cmap=\"jet\", fmt=\".3f\")\n",
    "plt.title(\"Average of Total Attention for Central Area by Transformation and AIO\")\n",
    "plt.ylabel(\"Transformation\")\n",
    "plt.xlabel(\"Weight File\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"average_attention_heatmap.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaee2cd",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-iqa-python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
