{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a688702",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from model.vit_for_small_dataset_custom import ViT\n",
    "from utils.imageset_handler import ImageQualityDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9cce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pth_files(directory_path):\n",
    "    \"\"\"\n",
    "    Find and return a list of full paths to .pth files in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The directory path to search for .pth files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of full paths to .pth files.\n",
    "    \"\"\"\n",
    "    pth_files = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pth\"):\n",
    "                pth_files.append(os.path.join(root, file))\n",
    "    return pth_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b2b95",
   "metadata": {},
   "source": [
    "# 1. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71892202",
   "metadata": {},
   "source": [
    "### 1.1 Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=256\n",
    "patch_size=16\n",
    "num_classes=5  # Number of classes for image quality levels\n",
    "dim=1024\n",
    "depth=6\n",
    "heads=16\n",
    "mlp_dim=2048\n",
    "emb_dropout=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12028a",
   "metadata": {},
   "source": [
    "### 1.2 Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=dim,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    emb_dropout=emb_dropout\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80426c",
   "metadata": {},
   "source": [
    "# 2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights_path = f'{results_path}/vit_model_20230821_121731_epoch_2of20_valLoss_1.572_valAcc_0.267_batchsize_64_lr_0.0_TestImg.pth'\n",
    "weights_dir = '/home/maxgan/WORKSPACE/UNI/BA/vision-transformer-for-image-quality-perception-of-individual-observers/results/weights/all_distored_imgs_1'\n",
    "# weights_path = f'/home/maxgan/WORKSPACE/UNI/BA/vision-transformer-for-image-quality-perception-of-individual-observers/results/weights/TEST/vit_model_20230821_120855_epoch_16of20_valLoss_7.457_valAcc_0.233_batchsize_64_lr_0.0_TestImg.pth'\n",
    "\n",
    "csv_file = '/home/maxgan/WORKSPACE/UNI/BA/vision-transformer-for-image-quality-perception-of-individual-observers/assets/Test/AccTestCsv/objectiveAccTest.csv'\n",
    "dataset_root =  '/home/maxgan/WORKSPACE/UNI/BA/vision-transformer-for-image-quality-perception-of-individual-observers/assets/Test/TestImg'\n",
    "batch_size = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd353bf",
   "metadata": {},
   "source": [
    "### 2.1 Add Augmentation (Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization parameters (mean and std)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the transformation including normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1222b5e",
   "metadata": {},
   "source": [
    "### 2.2 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e155d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your dataset loader and test dataset\n",
    "test_dataset = ImageQualityDataset(csv_file,dataset_root, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96485153",
   "metadata": {},
   "source": [
    "# 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4782965b",
   "metadata": {},
   "source": [
    "### 3.1 Evaluating best weights by calculating class probability, MSE of most likely class and weighted sum, mean entropy, accuracy and classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of different weight files\n",
    "weight_files = find_pth_files(weights_dir)\n",
    "\n",
    "results = []\n",
    "example_pred_results = []\n",
    "\n",
    "for weight_file in weight_files:\n",
    "    print(f'Weights-file: {os.path.basename(weight_file)} will be evaluated')\n",
    "    # Load the model with different weights\n",
    "    model.load_state_dict(torch.load(weight_file))\n",
    "    model.eval()\n",
    "\n",
    "    true_labels = []\n",
    "    test_preds = []\n",
    "    entropies = []\n",
    "    weighted_sums = []\n",
    "    kl_divs = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, (images, labels) in enumerate(test_loader, 0):\n",
    "            # images = images.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            print(f\"Example Prediction of Batch: {i}\")\n",
    "            outputs, _ = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "            # Convert logits to probabilities\n",
    "            probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "            # Format probabilities in a readble way\n",
    "\n",
    "\n",
    "            # Calculate Entropy\n",
    "            entropy_values = entropy(probabilities.numpy(),base=2, axis=1)\n",
    "            # Format entropies in a readble way\n",
    "            entropies.extend(entropy_values)\n",
    "\n",
    "            # Convert labels to one-hot encoded format\n",
    "            labels_one_hot = torch.zeros(probabilities.size(), dtype=torch.float32)\n",
    "            labels_one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "\n",
    "            # Calculate KL Divergence\n",
    "            kl_div = torch.nn.functional.kl_div(torch.log(probabilities), labels_one_hot, reduction='none')\n",
    "            kl_div = torch.sum(kl_div, dim=1).numpy()\n",
    "            kl_divs.extend(kl_div)\n",
    "\n",
    "            # Define weighting factors\n",
    "            weighting_factors = [0,1,2,3,4]\n",
    "            # Calculate the weighted sum of probabilities\n",
    "            weighted_sum = torch.sum(probabilities * torch.tensor(weighting_factors), dim=1).cpu().numpy()\n",
    "            # Format weighted sum in a readble way\n",
    "            weighted_sums.extend(weighted_sum)\n",
    "\n",
    "            # Example printout for the first batch (you can customize as needed)\n",
    "            if i == 0:\n",
    "                example_pred_result = {\n",
    "                    \"Weights File\": os.path.basename(weight_file),\n",
    "                    \"True Label\": labels.cpu().numpy()[i],\n",
    "                    \"Predicted Label\": preds.cpu().numpy()[i],\n",
    "                    \"Weighted Sum of Probability\": weighted_sum[i],\n",
    "                    \"Predicted Class Probability\": probabilities[i],\n",
    "                    \"Entropy Value\": entropy_values[i],\n",
    "                    \"KL Divergence\": kl_divs[i],\n",
    "                }\n",
    "                example_pred_results.append(example_pred_result)\n",
    "            print(f'True-Label: {labels.cpu().numpy()[0]}')\n",
    "            print(f'Predicted-Label: {preds.cpu().numpy()[0]}')\n",
    "            print(f'Weighted Sum of Probability: {round(weighted_sum[0],4)}')  # Gewichtete Summe der Wahrscheinlichkeiten\n",
    "            print(f'Predicted-Class-Probality: {[round(prob,4) for prob in probabilities[0].numpy()]}')\n",
    "            print(f'Entropy Value: {round(entropy_values[0],4)}') # High Value: spreading; Low Value: concentrated\n",
    "            print(f'KL Divergence: {round(kl_div[0],4)}\\n')\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the MSE of weighted sum and ground truth\n",
    "    mse_weighted = mean_squared_error(true_labels, weighted_sums)\n",
    "\n",
    "    # Calculate the MSE of most likely class and ground truth\n",
    "    mse = mean_squared_error(true_labels, test_preds)\n",
    "    \n",
    "    # Calculate the Mean Entropy\n",
    "    mean_entropy = np.mean(entropies)\n",
    "\n",
    "    # Calculate the Mean KL Divergence\n",
    "    mean_kl_div = np.mean(kl_divs)\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(true_labels, test_preds)\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(true_labels, test_preds)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    confusion = confusion_matrix(true_labels, test_preds)\n",
    "    \n",
    "    print('Model Summary:')\n",
    "    print(f'Weight: {os.path.basename(weight_file)}, Accuracy: {accuracy}, Mean Entropy: {mean_entropy}, Mean KL Div: {mean_kl_div:.4f}, weighted mean mse: {mse_weighted},\\nClassification Report:\\n{class_report}')\n",
    "    # Save confusion matrix as a figure\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(weight_file.replace(\".pth\", \"_confusion.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        \"Weights File\": os.path.basename(weight_file),\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"MSE\": mse,\n",
    "        \"MSE weighted\": mse_weighted,\n",
    "        \"Mean Entropy\": mean_entropy,\n",
    "        \"Mean KL Divergence\": mean_kl_div, \n",
    "        \"Classification Report\": class_report\n",
    "    })\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_path = os.path.join(weights_dir, \"model_comparison_results.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "# Save example printouts to a CSV file for this model\n",
    "example_printouts_df = pd.DataFrame(example_pred_results)\n",
    "example_printout_file = os.path.join(weights_dir, \"model_comparison_results_examples.csv\")\n",
    "example_printouts_df.to_csv(example_printout_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdb6fa",
   "metadata": {},
   "source": [
    "### 3.2 Plot Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c40497",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"/home/maxgan/WORKSPACE/UNI/BA/TIQ/assets/Test/AccTestCsv/shinyxAccTest20-01-2023.csv\"\n",
    "output_image_path = \"/home/maxgan/WORKSPACE/UNI/BA/TIQ/assets/Test/AccTestCsv/rating_distribution_shinyxAccTest.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74ad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(csv_file, header=None, skiprows=1)\n",
    "\n",
    "# Map rating values to their corresponding labels\n",
    "rating_labels = {\n",
    "    1: \"Bad\",\n",
    "    2: \"Insufficient\",\n",
    "    3: \"Fair\",\n",
    "    4: \"Good\",\n",
    "    5: \"Excellent\"\n",
    "}\n",
    "data[\"Rating_Label\"] = data[1].map(rating_labels)\n",
    "\n",
    "# Group data by Rating_Label and count occurrences\n",
    "class_counts = data[\"Rating_Label\"].value_counts().sort_index()\n",
    "# Calculate total number of images\n",
    "total_images = class_counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee564bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.plot(kind=\"bar\", color='skyblue')\n",
    "plt.title(\"Image Rating Distribution Person1 (shinyx)\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_image_path)\n",
    "plt.show()\n",
    "# Display the table\n",
    "print(\"Rating Distribution Table:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aec878",
   "metadata": {},
   "source": [
    "### 3.2 Plot Mean Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74487aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize and normalize attention scores to match the original image patch dimensions\n",
    "def resize_and_normalize_attention_maps(attention_maps, image_patch_size, image_size):\n",
    "    resized_and_normalized_attention_maps = []\n",
    "    for attention_map in attention_maps:\n",
    "        # print(f\"Function: {attention_map.shape}\")\n",
    "        # Resize the attention map to match image patch dimensions\n",
    "        resized_attention_map = np.zeros((image_size, image_size))\n",
    "        for i in range(attention_map.shape[0]): # seq_length\n",
    "            for j in range(attention_map.shape[1]): # seq_length\n",
    "                # Compute the coordinates in the resized attention map\n",
    "                x_start = i * image_patch_size\n",
    "                x_end = (i + 1) * image_patch_size\n",
    "                y_start = j * image_patch_size\n",
    "                y_end = (j + 1) * image_patch_size\n",
    "\n",
    "                # Resize the attention map and add it to the corresponding region\n",
    "                resized_attention_map[x_start:x_end, y_start:y_end] = attention_map[i, j]\n",
    "\n",
    "        # Normalize the attention map to range [0, 1]\n",
    "        min_value = np.min(resized_attention_map)\n",
    "        max_value = np.max(resized_attention_map)\n",
    "        normalized_attention_map = (resized_attention_map - min_value) / (max_value - min_value)\n",
    "\n",
    "        resized_and_normalized_attention_maps.append(normalized_attention_map)\n",
    "\n",
    "    return resized_and_normalized_attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df9655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = 'results/weights/all_distored_imgs_1/vit_model_20230911_064440_epoch_148of150_valLoss_0.108_valAcc_0.953_batchsize_128_lr_0.0_allDistorted.pth'\n",
    "\n",
    "image_size=256\n",
    "patch_size=16\n",
    "num_classes=5  # Number of classes for image quality levels\n",
    "dim=1024\n",
    "depth=6\n",
    "heads=16\n",
    "mlp_dim=2048\n",
    "emb_dropout=0.1\n",
    "\n",
    "model = ViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    dim=dim,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    mlp_dim=mlp_dim,\n",
    "    emb_dropout=emb_dropout,\n",
    "    # pool='mean'\n",
    ")\n",
    "print(model)\n",
    "\n",
    "model.load_state_dict(torch.load(weight_file))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6fbd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'assets/Test/TestImg/596ILSVRC2013_train_00009848.JPEG_I5_Q95.jpeg'\n",
    "image_path = '/home/maxgan/Downloads/vit_model_20230909_021136_epoch_89of150_valLoss_0.134_valAcc_0.941_batchsize_128_lr_0.0_allDistorted_confusion.jpg'\n",
    "\n",
    "image_org = Image.open(image_path)\n",
    "\n",
    "\n",
    "image = transform(image_org)\n",
    "with torch.no_grad():\n",
    "    out, att = model(image.unsqueeze(0))\n",
    "    print(f\"Out: {out.shape}\")\n",
    "    _, preds = torch.max(out, 1)\n",
    "    print(f\"Predicted Class: {preds.cpu().numpy()[0]}\")\n",
    "    probabilities = nn.functional.softmax(out, dim=1)\n",
    "    # Format probabilities in a readble way\n",
    "    formatted_probs = [[f'{p:.4f}' for p in prob_list] for prob_list in probabilities.numpy()]\n",
    "    print(f\"Probabilities: {formatted_probs[0]}\")    \n",
    "\n",
    "    # Define weighting factors\n",
    "    weighting_factors = [0,1,2,3,4]\n",
    "    # Calculate the weighted sum of probabilities\n",
    "    weighted_sum = torch.sum(probabilities * torch.tensor(weighting_factors), dim=1).cpu().numpy()\n",
    "    # Format weighted sum in a readble way\n",
    "    formatted_weighted_sum = [f'{sum:.4f}' for sum in weighted_sum]\n",
    "    print(f\"Predicted Class (weighted): {formatted_weighted_sum[0]}\")\n",
    "\n",
    "    print(f\"Attention - Shape: {att.shape}\") # batch, layers (depth), heads, sequence length, sequence length\n",
    "    attn_patches = att[:, :, :,1:, 1:] # No Class-Token\n",
    "    attn_patches = attn_patches.squeeze(0) # No Batch\n",
    "    att_mean = torch.mean(attn_patches, dim=1) # Mean of Heads\n",
    "\n",
    "    att_mean_scores = att_mean.cpu().numpy()\n",
    "\n",
    "    print(f\"Attention - Mean: {att_mean.shape}\") # layers (depth), sequence length, sequence length\n",
    "    # Overlay attention maps on the image patches\n",
    "   # Reverse the normalization\n",
    "    image = image.permute(1, 2, 0)  # Convert to HxWxC format\n",
    "    image = image * np.array(std) + np.array(mean)\n",
    "\n",
    "    print(f\"Image: {image.shape}\")\n",
    "    # Plot the original image with overlaid attention maps\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)  # Display the original image\n",
    "    # Resize and normalize attention maps\n",
    "    resized_and_normalized_attention_maps = resize_and_normalize_attention_maps(\n",
    "        att_mean_scores, patch_size, image_size\n",
    "    )\n",
    "    \n",
    "    resized_and_normalized_attention_maps = np.stack(resized_and_normalized_attention_maps, axis=0)\n",
    "    resized_and_normalized_attention_map = np.mean(resized_and_normalized_attention_maps, axis=0)\n",
    "    # Plot the original image with overlaid attention maps\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(resized_and_normalized_attention_map, alpha=0.7, cmap='viridis', interpolation='nearest')\n",
    "    plt.title(f'Mean Attention')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35088d3b",
   "metadata": {},
   "source": [
    "### Plot Attention Per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983dbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'assets/Test/TestImg/25519ILSVRC2014_train_00005904.JPEG_I3_Q22.jpeg'\n",
    "\n",
    "image = Image.open(image_path)\n",
    "\n",
    "image = transform(image)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out, att = model(image.unsqueeze(0))\n",
    "    _, preds = torch.max(out, 1)\n",
    "    print(f\"Predicted Class: {preds.cpu().numpy()[0]}\")\n",
    "    probabilities = nn.functional.softmax(out, dim=1)\n",
    "    print(f\"Probabilities: {probabilities.cpu().numpy()[0]}\")\n",
    "    print(f\"Attention - Shape: {att.shape}\") # batch, layers (depth), heads, sequence length, sequence length\n",
    "    attn_patches = att[:, :, :,1:, 1:] # No Class-Token\n",
    "    attn_patches = attn_patches.squeeze(0) # No Batch\n",
    "    att_mean = torch.mean(attn_patches, dim=1) # Mean of Heads\n",
    "    att_mean_scores = att_mean.cpu().numpy()\n",
    "\n",
    "    print(f\"Attention - Mean: {att_mean.shape}\") # layers (depth), sequence length, sequence length\n",
    "    # Overlay attention maps on the image patches\n",
    "    image = image.permute(1,2,0)\n",
    "    print(f\"Image: {image.shape}\")\n",
    "    # Plot the original image with overlaid attention maps\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)  # Display the original image\n",
    "    # Resize and normalize attention maps\n",
    "    resized_and_normalized_attention_maps = resize_and_normalize_attention_maps(\n",
    "        att_mean_scores, patch_size, image_size\n",
    "    )\n",
    "    # Overlay attention maps on the image patches\n",
    "    for i, attention_map in enumerate(resized_and_normalized_attention_maps):\n",
    "        print(f\"Resized Attention Shape: {attention_map.shape}\")\n",
    "        # Plot the original image with overlaid attention maps\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(image)\n",
    "        plt.imshow(attention_map, alpha=0.7, cmap='viridis', interpolation='nearest')\n",
    "        plt.title(f'Layer {i+1} Attention')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db24d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Resize attention maps\n",
    "resized_attention_maps = resize_and_normalize_attention_maps(att_mean_scores, patch_size, image_size)\n",
    "\n",
    "image_path = 'assets/Test/TestImg/25519ILSVRC2014_train_00005904.JPEG_I3_Q22.jpeg'\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image = transform(image)\n",
    "# Plot the original image with overlaid attention maps\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image.permute(1,2,0))  # Display the original image\n",
    "# Resize and normalize attention maps\n",
    "resized_and_normalized_attention_maps = resize_and_normalize_attention_maps(\n",
    "    att_mean_scores, patch_size, image_size\n",
    ")\n",
    "\n",
    "# Overlay attention maps on the image patches\n",
    "for i, attention_map in enumerate(resized_attention_maps):\n",
    "    # Plot the original image with overlaid attention maps\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image.permute(1,2,0))  # Display the original image\n",
    "    plt.imshow(attention_map, alpha=0.7, cmap='viridis', interpolation='nearest')\n",
    "    plt.title(f'Layer {i+1} Attention')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
