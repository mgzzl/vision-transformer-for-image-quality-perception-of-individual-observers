{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a688702",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix\n",
    "from misc.helpers import find_model_weights, calculate_label_distributions, prev_img, prev_img_gray, trans_norm2tensor, find_csv_files, get_image_paths_from_csv, get_image_paths_from_dir, create_vit_model\n",
    "from misc.visualization import *\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from model.vit_for_small_dataset import ViT\n",
    "from utils.imageQualityDataset import ImageQualityDataset\n",
    "from utils.imageAttentionGlobalAvgDataset import ImageAttentionGlobalAvgDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b2b95",
   "metadata": {},
   "source": [
    "# 1. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=256\n",
    "patch_size=16\n",
    "num_classes=5\n",
    "depth = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_model()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80426c",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'assets/Test/Obs0.csv'\n",
    "dataset_root =  'assets/Test/DSX'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd353bf",
   "metadata": {},
   "source": [
    "### 2.1 Add Augmentation (Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization parameters (mean and std)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the transformation including normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1222b5e",
   "metadata": {},
   "source": [
    "### 2.2 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e155d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset loader and test dataset\n",
    "test_dataset = ImageQualityDataset(csv_file,dataset_root, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4427511",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_to_check = 'assets/Test/DSX/45737ILSVRC2014_train_00060591.JPEG_I4_Q31.jpeg'\n",
    "label = test_dataset.get_label_by_image_path(image_path_to_check)\n",
    "print(f\"The label for image '{os.path.basename(image_path_to_check)}' is: {label+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96485153",
   "metadata": {},
   "source": [
    "# 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate single weight (AIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different weight files\n",
    "weight_file = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO2.pth'\n",
    "results = []\n",
    "example_pred_results = []\n",
    "\n",
    "print(f'Weights-file: {os.path.basename(weight_file)} will be evaluated')\n",
    "# Load the model with different weights\n",
    "model = create_vit_model(weights_path=weight_file)\n",
    "model.eval()\n",
    "\n",
    "# init result lists\n",
    "true_labels = []\n",
    "test_preds = []\n",
    "entropies = []\n",
    "true_entropies =[]\n",
    "weighted_sums = []\n",
    "kl_divs = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, (images, image_paths, labels) in enumerate(test_loader, 0):\n",
    "        # images = images.to(device)\n",
    "        # labels = labels.to(device)\n",
    "        print(f\"Example Prediction of Batch: {i}\")\n",
    "        outputs = model(images)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Calculate the true distribution\n",
    "        true_distributions = calculate_label_distributions(labels,device='cpu')\n",
    "        # Get predicition by the maximum probability\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "        formatted_probabilities = [\"{:.4f}\".format(prob) for prob in probabilities[0]]\n",
    "        # print(f\"Predicted Probabilities:{formatted_probabilities}\")\n",
    "        # print(f'Predicted Label: {preds[0]}')\n",
    "\n",
    "\n",
    "        # Calculate Entropy\n",
    "        entropy_values = entropy(probabilities.numpy(),base=np.exp(1), axis=1)\n",
    "        true_entropy_values = entropy(true_distributions.numpy(),base=np.exp(1), axis=1)\n",
    "        # Format entropies in a readble way\n",
    "        entropies.extend(entropy_values)\n",
    "        true_entropies.extend(entropy_values)\n",
    "        # print(f'Mean Entropie of batch: {np.mean(entropy_values)}')\n",
    "\n",
    "        # Calculate KL Divergence\n",
    "        kl_div = torch.nn.functional.kl_div(torch.log(probabilities), true_distributions, reduction='batchmean')\n",
    "        # print(f'KL-Divergence batchmean: {kl_div}')\n",
    "        kl_divs.append(kl_div.item())\n",
    "        \n",
    "        # Define weighting factors\n",
    "        weighting_factors = [0,1,2,3,4]\n",
    "        # Calculate the weighted sum of probabilities\n",
    "        weighted_sum = torch.sum(probabilities * torch.tensor(weighting_factors), dim=1).cpu().numpy()\n",
    "        # Format weighted sum in a readble way\n",
    "        weighted_sums.extend(weighted_sum)\n",
    "        # Example printout for the first batch\n",
    "        if i <=2:\n",
    "            example_pred_result = {\n",
    "                \"Weights File\": os.path.basename(weight_file),\n",
    "                \"Image Name\": os.path.basename(image_paths[i]),\n",
    "                \"True Label\": labels.cpu().numpy()[i],\n",
    "                \"Predicted Label\": preds.cpu().numpy()[i],\n",
    "                \"Weighted Sum of Probability\": weighted_sum[i],\n",
    "                \"True Probability Distribution\": true_distributions[i].cpu().numpy().tolist(),\n",
    "                \"Predicted Probability Distribution\": probabilities[i].cpu().numpy().tolist(),\n",
    "                \"Entropy Value\": entropy_values[i],\n",
    "                \"True Entropy Value\": true_entropy_values[i],\n",
    "                \"KL Divergence (batch-mean)\": kl_divs[i],\n",
    "            }\n",
    "            example_pred_results.append(example_pred_result)\n",
    "        print(f'True-Label: {labels.cpu()[0]}')\n",
    "        print(f'Predicted-Label: {preds.cpu().numpy()[0]}')\n",
    "        print(f'Weighted Sum of Probability: {round(weighted_sum[0],4)}')  # Weighted Sum of Prob\n",
    "        print(f'Predicted Probality Distribution: {[round(prob,4) for prob in probabilities[0].numpy()]}')\n",
    "        print(f'True Probality Distribution: {true_distributions.cpu().numpy()[0]}')\n",
    "        print(f'Entropy Value: {round(entropy_values[0],4)}') # High Value: spreading; Low Value: concentrated\n",
    "        print(f'True Entropy Value: {round(true_entropy_values[0],4)}') # High Value: spreading; Low Value: concentrated\n",
    "        print(f'KL Divergence (batch-mean): {round(kl_div.item(),4)}\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the MSE of weighted sum and ground truth\n",
    "mse_weighted = mean_squared_error(true_labels, weighted_sums)\n",
    "\n",
    "# Calculate the MSE of most likely class and ground truth\n",
    "mse = mean_squared_error(true_labels, test_preds)\n",
    "\n",
    "# Calculate the Mean Entropy\n",
    "mean_entropy = np.mean(entropies)\n",
    "\n",
    "# Calculate the Mean KL Divergence\n",
    "mean_kl_div = np.mean(kl_divs)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(true_labels, test_preds)\n",
    "target_names  = [\"bad\", \"poor\", \"fair\", \"good\", \"excellent\"]\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(true_labels, test_preds, target_names=target_names)\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion = confusion_matrix(true_labels, test_preds)\n",
    "print('#'*50)\n",
    "print('model summary:')\n",
    "# Save confusion matrix as a figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(weight_file.replace(\".pth\", \"_confusion.png\"))\n",
    "plt.close()\n",
    "result = {\n",
    "    \"Weights File\": os.path.basename(weight_file),\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"MSE\": mse,\n",
    "    \"MSE weighted\": mse_weighted,\n",
    "    \"Mean Entropy\": mean_entropy,\n",
    "    \"Mean KL Divergence\": mean_kl_div, \n",
    "    \"Classification Report\": class_report\n",
    "}\n",
    "# Store the results\n",
    "results.append(result)\n",
    "# Print summary\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print('#'*50)\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_path = weight_file.replace(\".pth\", \"_model_comparison_results.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "# Save example printouts to a CSV file for this model\n",
    "example_printouts_df = pd.DataFrame(example_pred_results)\n",
    "example_printout_file = weight_file.replace(\".pth\", \"_model_comparison_results_examples.csv\")\n",
    "example_printouts_df.to_csv(example_printout_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"assets/Test/DSX\"\n",
    "csv_dir = \"assets/Test\"\n",
    "csv_files = [os.path.join(csv_dir, f'Obs{i}.csv') for i in range(1,6)]\n",
    "weight_files = [f'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO{i}.pth'for i in range(6)]\n",
    "\n",
    "print(csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Initialize dataset loader and test dataset\n",
    "    test_dataset = ImageQualityDataset(csv_file,dataset_root, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    print(f\"loaded dataset due to {os.path.basename(csv_file)}\")\n",
    "    example_pred_results = []\n",
    "    aio_acc = []\n",
    "\n",
    "    for weight_file in weight_files:\n",
    "        print(f'Weights-file: {os.path.basename(weight_file)} will be evaluated')\n",
    "        # Load the model with different weights\n",
    "        model.load_state_dict(torch.load(weight_file))\n",
    "        model.eval()\n",
    "        # aio_idx = int(''.join(filter(str.isdigit, os.path.basename(weight_file))))\n",
    "        # init result lists\n",
    "        true_labels = []\n",
    "        test_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images,image_paths, labels) in enumerate(test_loader, 0):\n",
    "                # images = images.to(device)\n",
    "                # labels = labels.to(device)\n",
    "                print(f\"Example Prediction of Batch: {i}\")\n",
    "                outputs = model(images)\n",
    "                true_labels.extend(labels)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "                 \n",
    "                if i != 0:\n",
    "                    true_distributions = calculate_label_distributions(labels,device='cpu')\n",
    "                else:\n",
    "                    true_distributions = torch.nn.functional.one_hot(labels, num_classes).float()\n",
    "\n",
    "                # Get predicition by the maximum probability\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                print(f'Image Name: {os.path.basename(image_paths[i])}')\n",
    "                print(f'True-Label: {labels.cpu()[0]}')\n",
    "                print(f'Predicted-Label: {preds.cpu().numpy()[0]}')\n",
    "                print(f'Predicted Probality Distribution: {[round(prob,4) for prob in probabilities[0].numpy()]}')\n",
    "                print(f'True Probality Distribution: {true_distributions.cpu().numpy()[0]}')\n",
    "\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        accuracy = accuracy_score(true_labels, test_preds)\n",
    "\n",
    "        print('\\n')\n",
    "        print('#'*50)\n",
    "        print('model summary:')\n",
    "        result = {\n",
    "            \"Weights File\": os.path.basename(weight_file),\n",
    "            \"Accuracy (accaptable)\": round(accuracy,2),\n",
    "        }\n",
    "        aio_acc.append(accuracy)\n",
    "        # Print summary\n",
    "        for key, value in result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print('#'*50)\n",
    "        print('\\n')\n",
    "\n",
    "        target_names  = [\"bad\", \"poor\", \"fair\", \"good\", \"excellent\"]\n",
    "        # Generate confusion matrix\n",
    "        confusion = confusion_matrix(true_labels, test_preds)\n",
    "        \n",
    "        # Create a DataFrame from the confusion matrix with target names\n",
    "        # confusion_df = pd.DataFrame(confusion, index=target_names, columns=target_names)\n",
    "\n",
    "        # Ensure the confusion matrix has the correct shape (5x5 in this case)\n",
    "        expected_shape = (len(target_names), len(target_names))\n",
    "\n",
    "        # Create a DataFrame with zeros and the correct shape\n",
    "        confusion_df = pd.DataFrame(0, index=target_names, columns=target_names)\n",
    "\n",
    "        # Update the DataFrame with the values from the provided confusion matrix\n",
    "        confusion_df.loc[confusion_df.index[:len(confusion)], confusion_df.columns[:len(confusion)]] = confusion\n",
    "\n",
    "\n",
    "\n",
    "        obs_name, _ = os.path.splitext(os.path.basename(csv_file))\n",
    "        confusion_path = weight_file.replace(\".pth\", f\"_{obs_name}_confusion.png\")\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(confusion_df, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig(confusion_path)\n",
    "        plt.close()\n",
    "\n",
    "    obs.append(aio_acc)\n",
    "\n",
    "\n",
    "# Create a DataFrame for the obs list with proper headers and index (cross comparison)\n",
    "cc_df = pd.DataFrame(obs, columns=[f\"AIO{i}\" for i in range(6)], index=[f\"Obs{i}\" for i in range(6)])\n",
    "\n",
    "# Specify the path to save the obs table as a CSV file\n",
    "cc_path = os.path.join(os.path.dirname(weight_files[0]),\"cross_comparison.csv\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "cc_df.to_csv(cc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acceptable Ratio Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"assets/Test\"\n",
    "csv_files = [f'assets/Test/Obs{i}.csv'for i in range(6)]\n",
    "# List of weight files\n",
    "weights_dir = \"results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL\"\n",
    "weight_files = [f'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO{i}.pth'for i in range(6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [[] for _ in range(len(csv_files))]\n",
    "for csv_file in csv_files:\n",
    "    # Initialize dataset loader and test dataset\n",
    "    test_dataset = ImageQualityDataset(csv_file,dataset_root, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    obs_idx = int(''.join(filter(str.isdigit, csv_file)))-1\n",
    "    example_pred_results = []\n",
    "    aio_acc = [0] * len(weight_files)\n",
    "\n",
    "    for weight_file in weight_files:\n",
    "        print(f'Weights-file: {os.path.basename(weight_file)} will be evaluated')\n",
    "        # Load the model with different weights\n",
    "        model.load_state_dict(torch.load(weight_file))\n",
    "        model.eval()\n",
    "        aio_idx = int(''.join(filter(str.isdigit, os.path.basename(weight_file))))-1\n",
    "        # init result lists\n",
    "        true_labels = []\n",
    "        test_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images,image_paths, labels) in enumerate(test_loader, 0):\n",
    "                # images = images.to(device)\n",
    "                # labels = labels.to(device)\n",
    "                print(f\"Example Prediction of Batch: {i}\")\n",
    "                outputs = model(images)\n",
    "                true_labels.extend(labels)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "                 \n",
    "                if obs_idx != 0 and aio_idx !=0 or obs_idx != 0:\n",
    "                    # Calculate the true distribution\n",
    "                    true_distributions = calculate_label_distributions(labels,device='cpu')\n",
    "                else:\n",
    "                    true_distributions = torch.nn.functional.one_hot(labels, num_classes).float()\n",
    "\n",
    "                # Get predicition by the maximum probability\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                print(f'Image Name: {os.path.basename(image_paths[i])}')\n",
    "                print(f'True-Label: {labels.cpu()[0]}')\n",
    "                print(f'Predicted-Label: {preds.cpu().numpy()[0]}')\n",
    "                print(f'Predicted Probality Distribution: {[round(prob,4) for prob in probabilities[0].numpy()]}')\n",
    "                print(f'True Probality Distribution: {true_distributions.cpu().numpy()[0]}')\n",
    "\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        # accuracy = accuracy_score(true_labels, test_preds)\n",
    "        correct_predictions = sum(1 for true, pred in zip(true_labels, test_preds) if true == pred or abs(true - pred) == 1)\n",
    "        total_predictions = len(true_labels)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        print('\\n')\n",
    "        print('#'*50)\n",
    "        print('model summary:')\n",
    "        result = {\n",
    "            \"Weights File\": os.path.basename(weight_file),\n",
    "            \"Accuracy (accaptable)\": round(accuracy,2),\n",
    "        }\n",
    "        aio_acc[aio_idx]=accuracy\n",
    "        # Print summary\n",
    "        for key, value in result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print('#'*50)\n",
    "        print('\\n')\n",
    "    obs[obs_idx]=aio_acc\n",
    "\n",
    "\n",
    "# Create a DataFrame for the obs list with proper headers and index\n",
    "obs_df = pd.DataFrame(obs, columns=[f\"AIO{i}\" for i in range(1,6)], index=[f\"Obs{i}\" for i in range(1,6)])\n",
    "\n",
    "# Specify the path to save the obs table as a CSV file\n",
    "obs_path = os.path.join(weights_dir,\"cross_comparison_acceptable_ratio.csv\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "obs_df.to_csv(obs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_model()\n",
    "batch_size = 128\n",
    "\n",
    "weights_dir = \"results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL\"\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1,6)]\n",
    "\n",
    "# dir with images\n",
    "images_dir = \"assets/Test/DSX\"\n",
    "\n",
    "# image_paths = get_image_paths_from_dir(images_dir)\n",
    "# image_paths = get_image_paths_from_csv('assets/Test/DSX/global_avg.csv', 5)\n",
    "\n",
    "# select images manually\n",
    "filenames = ['8034ILSVRC2013_train_00034320.JPEG_I4_Q50.jpeg', '10451ILSVRC2013_train_00058547.JPEG_I1_Q2.jpeg', '4692ILSVRC2013_train_00079353.JPEG_I3_Q22.jpeg', '30442ILSVRC2014_train_00005639.JPEG_I1_Q3.jpeg', '4337ILSVRC2013_train_00018603.JPEG_I1_Q8.jpeg']\n",
    "image_paths = [os.path.join(images_dir, filename) for filename in filenames]\n",
    "\n",
    "print(weight_files)\n",
    "print(image_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### True Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_distribution = calculate_label_distributions([1],device=device).cpu().numpy()[0]\n",
    "# Create an index for the x-axis\n",
    "index = range(1, len(true_distribution) + 1)\n",
    "\n",
    "# Plot the probability distribution\n",
    "plt.bar(index, true_distribution, tick_label=index)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Probabilities')\n",
    "# Add text labels for each bar\n",
    "for i in range(len(true_distribution)):\n",
    "    plt.text(index[i], true_distribution[i], f'{true_distribution[i]:.2f}', ha='center', va='bottom')\n",
    "plt.title('Probability Distribution')\n",
    "plt.savefig('Probability Distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicted Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Init Matplotlib-Fig\n",
    "num_weights = len(weight_files)\n",
    "num_images = len(image_paths)\n",
    "fig, axes = plt.subplots(num_images, num_weights+1, figsize=(30, 20))\n",
    "\n",
    "for i in range(len(weight_files)+1):\n",
    "    if i != 0:\n",
    "        model.load_state_dict(torch.load(weight_files[i-1]))\n",
    "\n",
    "    for j, image_path in enumerate(image_paths):\n",
    "        ax = axes[j,i]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if i == 0:  # Only the first column should have images\n",
    "            # Load the image and display it on the y-axis\n",
    "            img = prev_img(image, image_size)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title('Original Image')\n",
    "        else:\n",
    "            img_tensor = trans_norm2tensor(image,image_size)\n",
    "            img = img_tensor.unsqueeze(0).to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                output= model(img)\n",
    "\n",
    "            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]            # Adjust x-axis values\n",
    "            x = np.arange(1, len(probabilities) + 1)\n",
    "            ax.bar(x, probabilities)\n",
    "            ax.set_title(f'{os.path.splitext(os.path.basename(weight_files[i-1]))[0]}')\n",
    "            # Set Y-scale from 1 to 0\n",
    "            ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(probabilities, num_classes=5):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation of a distribution represented by probabilities.\n",
    "\n",
    "    This function computes the standard deviation of a distribution given a list of probabilities\n",
    "    for each class. It assumes that the probabilities are indexed by class label, starting from 1\n",
    "    up to the specified number of classes.\n",
    "\n",
    "    Parameters:\n",
    "        probabilities (list): A list of probabilities for each class.\n",
    "        num_classes (int): The total number of classes. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        float: The standard deviation of the distribution.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if the number of probabilities matches the number of classes\n",
    "    if len(probabilities) != num_classes:\n",
    "        raise ValueError(\"Number of probabilities and classes should be the same.\")\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    probabilities_tensor = torch.tensor(probabilities, dtype=torch.float32)\n",
    "    values_tensor = torch.arange(1, num_classes + 1)\n",
    "\n",
    "    # Calculate the mean (expectation) of the distribution\n",
    "    mean = torch.sum(probabilities_tensor * values_tensor)\n",
    "\n",
    "    # Calculate the squared difference of each value from the mean\n",
    "    squared_diff = (values_tensor - mean) ** 2\n",
    "\n",
    "    # Calculate the weighted sum of squared differences\n",
    "    weighted_sum = torch.sum(probabilities_tensor * squared_diff)\n",
    "\n",
    "    # Calculate the standard deviation as the square root of the weighted sum\n",
    "    std_deviation = torch.sqrt(weighted_sum)\n",
    "\n",
    "    return std_deviation.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ad4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_probabilities(label, predicted_probabilities, ax):\n",
    "    \"\"\"\n",
    "    Compare predicted probabilities with true distribution for a given label.\n",
    "\n",
    "    This function visualizes the predicted probabilities and the true distribution\n",
    "    for a given label using a bar chart. The predicted probabilities are plotted\n",
    "    in orange, while the true distribution is plotted in black.\n",
    "\n",
    "    Parameters:\n",
    "        label (int): The true label for comparison.\n",
    "        predicted_probabilities (list or numpy.ndarray): Predicted probabilities for each class.\n",
    "        ax (matplotlib.axes.Axes): The matplotlib axes object for plotting.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Calculate the true distribution for the given label\n",
    "    true_distribution = calculate_label_distributions(label, device=device)\n",
    "    true_distribution = true_distribution.cpu().numpy()[0]\n",
    "\n",
    "    # Set the offset for better visualization\n",
    "    offset = 0.35\n",
    "\n",
    "    # Plot the predicted probabilities and true distribution as bar charts\n",
    "    ax.bar(np.arange(len(predicted_probabilities)), predicted_probabilities, width=0.35, label='predicted', align='center', color='orange')\n",
    "    ax.bar(np.arange(len(true_distribution)) + offset, true_distribution, width=0.35, label='true', align='center', color='black')\n",
    "\n",
    "    # Set plot limits, labels, and ticks\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_xticks(np.arange(len(true_distribution)) + offset/2)\n",
    "    ax.set_xticklabels(np.arange(len(true_distribution)))\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = 'assets/Test'\n",
    "csv_files = [os.path.join(csv_dir, f'Obs{i}.csv') for i in range(1,6)]\n",
    "dataset_root = 'assets/Test/DSX'\n",
    "num_weights = len(weight_files)\n",
    "num_images = len(image_paths)\n",
    "fig, axes = plt.subplots(num_images, num_weights+1, figsize=(4.5 * num_weights+1, 3 * num_images))\n",
    "\n",
    "for i in range(len(weight_files)+1):\n",
    "    if i != 0:\n",
    "        model.load_state_dict(torch.load(weight_files[i-1]))\n",
    "        dataset = ImageQualityDataset(csv_files[i-1], dataset_root, transform=transform)\n",
    "    for j, image_path in enumerate(image_paths):\n",
    "        ax = axes[j,i]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if i == 0:  # Only the first column should have images\n",
    "            # Load the image and display it on the y-axis\n",
    "            img = prev_img(image, image_size)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title('Original Image')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            img_tensor = trans_norm2tensor(image,image_size)\n",
    "            img = img_tensor.unsqueeze(0).to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                output= model(img)\n",
    "\n",
    "            label = dataset.get_label_by_image_path(os.path.join(dataset_root, os.path.basename(image_path)))\n",
    "            label_tensor = torch.tensor(label).unsqueeze(0).cpu().numpy()\n",
    "\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            std_deviation_result = standard_deviation(probabilities[0].cpu().numpy())\n",
    "            print(f\"Standard deviation for image {j+1} and AIO {i}: {std_deviation_result}\")\n",
    "            compare_probabilities(label_tensor, probabilities[0].cpu().numpy(), ax)\n",
    "            ax.set_title(f'{os.path.splitext(os.path.basename(weight_files[i-1]))[0]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "depth = 6\n",
    "num_classes = 5\n",
    "patch_size = 16\n",
    "\n",
    "model = create_vit_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b7327",
   "metadata": {},
   "source": [
    "#### Plot Attention Map of Single AIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe976c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO5.pth'\n",
    "# image_path = 'assets/work_imgs/fg_bg/18387ILSVRC2013_train_00086939.JPEG_I5_Q60.jpeg'\n",
    "image_path = image_path_to_check\n",
    "layer_idx = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = create_vit_model(weights_path=weight_file)\n",
    "model.eval()\n",
    "image = Image.open(image_path)\n",
    "img_pre = trans_norm2tensor(image, image_size)\n",
    "_, attention = get_attention_maps(model, img_pre, patch_size, device)\n",
    "print(attention.shape)\n",
    "n_heads = attention.shape[1]\n",
    "n_layers = attention.shape[0]\n",
    "\n",
    "img_pre = prev_img(image, image_size)\n",
    "img_gray = prev_img_gray(image, image_size)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 5))  # Create subplots\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Plot the original image\n",
    "axes[0].imshow(img_pre)\n",
    "axes[0].set_title(\"Original Image\", fontsize=20, fontname='cmr10')\n",
    "\n",
    "# Plot the grayscale image with heatmap overlay for Median\n",
    "layer_attention = attention[layer_idx]\n",
    "print(layer_attention.shape)\n",
    "\n",
    "layer_mean = np.mean(layer_attention, axis=0)\n",
    "layer_mean_norm = normalize_attention_maps(layer_mean)\n",
    "\n",
    "heatmap = sns.heatmap(layer_mean_norm, cmap=\"inferno\", alpha=0.7, ax=axes[1])\n",
    "axes[1].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "axes[1].set_title(\"Attention\", fontsize=20, fontname='cmr10')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Attention across all layers and all heads (no comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064041b",
   "metadata": {},
   "source": [
    "Single AIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO0.pth'\n",
    "image_path = 'assets/work_imgs/focus/637ILSVRC2013_train_00009458.JPEG_I4_Q37.jpeg'\n",
    "\n",
    "model = create_vit_model(weights_path=weight_file)\n",
    "model.eval()\n",
    "image = Image.open(image_path)\n",
    "visualize_all_layer_head_attention_maps(model, image, image_size, patch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac572069",
   "metadata": {},
   "source": [
    "Multiple AIOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'assets/work_imgs/focus/637ILSVRC2013_train_00009458.JPEG_I4_Q37.jpeg'\n",
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(0, 6)]\n",
    "\n",
    "for weight_file in weight_files:\n",
    "    model = create_vit_model(weights_path=weight_file)\n",
    "    model.eval()\n",
    "    image = Image.open(image_path)\n",
    "    visualize_all_layer_head_attention_maps(model, image, image_size, patch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c4057",
   "metadata": {},
   "source": [
    "#### Plot Attention across specific layer (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(6)]\n",
    "\n",
    "images_dir = 'assets/work_imgs/fg_bg'\n",
    "image_paths = get_image_paths_from_dir(images_dir)\n",
    "\n",
    "output_dir = 'results/Attention_maps/comparisons'\n",
    "\n",
    "layer_idx = -1\n",
    "\n",
    "plot_attention_maps_comparison(weight_files,image_paths, image_size, patch_size,output_dir, layer_idx,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot mean head of all layers for every AIO (compare layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7046662",
   "metadata": {},
   "source": [
    "No sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(6)]\n",
    "images_dir = 'assets/work_imgs/blurry'\n",
    "image_paths = get_image_paths_from_dir(images_dir)\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path)\n",
    "    get_attention_maps_across_weights(model, image, image_size, patch_size, depth, weight_files, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8b7d3",
   "metadata": {},
   "source": [
    "Sub avg (in comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1,6)]\n",
    "images_dir = 'assets/work_imgs/focus'\n",
    "num_files = 5\n",
    "image_paths = get_image_paths_from_dir(images_dir,num_files)\n",
    "depth = 6\n",
    "\n",
    "csv_file_path = 'assets/Test/DSX/global_avg.csv'\n",
    "# image_paths = get_image_paths_from_csv(csv_file_path, num_files)\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    get_attention_maps_with_deviation(img, weight_files, image_size,depth, patch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d709fe4",
   "metadata": {},
   "source": [
    "#### Plot Attention of Images sorted by global avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5710f6",
   "metadata": {},
   "source": [
    "Load Dataset to get global average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "images_dir = 'assets/Test/DSX'\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1, 6)]\n",
    "layer_idx = -1\n",
    "\n",
    "# Create a DataLoader\n",
    "global_avg_dataset = ImageAttentionGlobalAvgDataset(images_dir, weight_files, image_size, patch_size, layer_idx, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f89210",
   "metadata": {},
   "source": [
    "Write for each image the global avg and path to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = os.path.join(images_dir, 'global_avg.csv')\n",
    "\n",
    "global_avg_dataset.write_to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74723bea",
   "metadata": {},
   "source": [
    "Get the top 5 images with the highest difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43624ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"test: {global_avg_dataset[0]}\")\n",
    "top_5 = global_avg_dataset.get_top_global_avg(5,True)\n",
    "# print(f\"Global Avg: {top_5}\")\n",
    "for i in range(len(top_5)):\n",
    "    fieldnames = ['filename', 'global_avg'] \n",
    "    selected_fields = {key: top_5[i][key] for key in fieldnames}\n",
    "    print(selected_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df080a",
   "metadata": {},
   "source": [
    "Get the top 5 images with the lowest difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_neg = global_avg_dataset.get_top_global_avg(5,False)\n",
    "\n",
    "for i in range(len(top_5_neg)):\n",
    "    fieldnames = ['filename', 'global_avg'] \n",
    "    selected_fields = {key: top_5_neg[i][key] for key in fieldnames}\n",
    "    print(selected_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot attention of images with highest global avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'results/Attention_maps/comparisons'\n",
    "weights_dir = '/home/maxgan/WORKSPACE/UNI/BA/vision-transformer-for-image-quality-perception-of-individual-observers/results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "images_dir = 'assets/Test/DSX'\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1, 6)]\n",
    "layer_idx = -1\n",
    "\n",
    "image_paths = []\n",
    "\n",
    "for i in range(len(top_5)):\n",
    "    filename = top_5[i]['filename']\n",
    "    global_avg = top_5[i]['global_avg']\n",
    "    image_paths.append(os.path.join(images_dir,filename))\n",
    "    print(f\"{filename} has a global average of {global_avg}\")\n",
    "\n",
    "filenames = ['8034ILSVRC2013_train_00034320.JPEG_I4_Q50.jpeg', '10451ILSVRC2013_train_00058547.JPEG_I1_Q2.jpeg', '4692ILSVRC2013_train_00079353.JPEG_I3_Q22.jpeg', '30442ILSVRC2014_train_00005639.JPEG_I1_Q3.jpeg', '4337ILSVRC2013_train_00018603.JPEG_I1_Q8.jpeg']\n",
    "# image_paths = [os.path.join(images_dir, filename) for filename in filenames]\n",
    "plot_attention_maps_comparison(weight_files, image_paths, image_size, patch_size,output_dir, layer_idx, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9507d",
   "metadata": {},
   "source": [
    "Plot attention of images with lowest global avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'results/Attention_maps/comparisons'\n",
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1, 6)]\n",
    "layer_idx = 5\n",
    "\n",
    "# csv_file_path = 'assets/work_imgs/fg_bg/global_avg.csv'\n",
    "# image_paths = get_image_paths_from_csv(csv0_file_path, num_files)\n",
    "# num_files = 5\n",
    "image_paths = []\n",
    "\n",
    "for i in range(len(top_5_neg)):\n",
    "    filename = top_5_neg[i]['filename']\n",
    "    global_avg = top_5_neg[i]['global_avg']\n",
    "    image_paths.append(os.path.join(images_dir,filename))\n",
    "    print(f\"{filename} has a global average of {global_avg}\")\n",
    "plot_attention_maps_comparison(weight_files, image_paths, image_size, patch_size,output_dir, layer_idx, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
