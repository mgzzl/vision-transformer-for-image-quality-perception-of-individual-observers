{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a688702",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3036095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report, confusion_matrix\n",
    "from misc.helpers import find_model_weights, calculate_label_distributions, prev_img, prev_img_gray, trans_norm2tensor, find_csv_files, get_image_paths_from_csv, get_image_paths_from_dir, get_image_filenames_by_label, create_vit_model\n",
    "from misc.visualization import *\n",
    "from scipy.stats import entropy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from model.vit_for_small_dataset import ViT\n",
    "from utils.imageQualityDataset import ImageQualityDataset\n",
    "from utils.imageAttentionGlobalAvgDataset import ImageAttentionGlobalAvgDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b2b95",
   "metadata": {},
   "source": [
    "# 1. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=256\n",
    "patch_size=16\n",
    "num_classes=5\n",
    "depth = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_model()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc import transformations\n",
    "\n",
    "# Load the images and apply the transformation\n",
    "resize_crop = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(256)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization parameters (mean and std)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the transformation including normalization\n",
    "img2tensor_normalize = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "# Define the transformation including normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055263b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img_gray(img: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Process the image for previewing purposes with grayscale and contrast adjustment.\n",
    "\n",
    "    This function applies transformations to resize and center crop the input image to a specified size.\n",
    "    It then converts the image to grayscale and enhances the contrast by scaling the luminance values.\n",
    "    \n",
    "    Parameters:\n",
    "    img (PIL.Image): The input image.\n",
    "\n",
    "    Returns:\n",
    "    img_bw_contrast_rgb (PIL.Image): The processed image with enhanced contrast in RGB format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    img_bw = img.convert('L')\n",
    "\n",
    "    # Convert grayscale image to YCbCr color space\n",
    "    img_ycbcr = img_bw.convert('YCbCr')\n",
    "    y, cb, cr = img_ycbcr.split()\n",
    "    # Convert Y channel to NumPy arrays\n",
    "    y_array = np.array(y)\n",
    "\n",
    "    # Perform contrast adjustment on the Y component\n",
    "    y_array = np.clip(y_array * 2.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Merge the adjusted Y, Cb, Cr components back into an image\n",
    "    img_bw_contrast = Image.merge('YCbCr', (Image.fromarray(y_array), cb, cr))\n",
    "\n",
    "    # Convert the image back to RGB for display\n",
    "    img_bw_contrast_rgb = img_bw_contrast.convert('RGB')\n",
    "    \n",
    "    return img_bw_contrast_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Images of I5 class (example filename: 1691ILSVRC2013_train_00007339.JPEG_I5_Q60.jpeg)\n",
    "# 1. Load the CSV file\n",
    "csv_file = 'assets/Test/Obs0.csv'\n",
    "\n",
    "# 2. Read the CSV file and store the image paths in a list\n",
    "# Extract the directory path from the CSV file path\n",
    "img_directory_path = 'assets/Test/DSX'\n",
    "\n",
    "# Get the list of image paths from the CSV file\n",
    "image_paths = get_image_filenames_by_label(csv_file, label=5, img_directory_path=img_directory_path)\n",
    "print(image_paths)\n",
    "\n",
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "\n",
    "weight_paths = find_model_weights(weights_dir) # Example path: 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO1.pth'\n",
    "\n",
    "\n",
    "output_dir = 'results/Attention_maps/comparisons/blur_patches'\n",
    "\n",
    "crop_size = (100,100)\n",
    "\n",
    "img_idx = 54 # 50, 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b51ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the image paths\n",
    "image_paths = []\n",
    "\n",
    "# Read the CSV file and extract the image filenames\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        image_filename = row[0]\n",
    "        vote = row[1]\n",
    "        # fitler only images with vote of 5\n",
    "        if int(vote) == 5:\n",
    "            image_path = os.path.join(img_directory_path, image_filename)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "print(image_paths[img_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde70e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [\"assets/Test/DSX/37344ILSVRC2014_train_00031300.JPEG_I5_Q88.jpeg\", \"assets/Test/DSX/31575ILSVRC2014_train_00030739.JPEG_I5_Q100.jpeg\"]  # List of image file paths or PIL images\n",
    "# images = image_paths\n",
    "transform_func = transformations.apply_compression_patch  # or apply_blur_patch\n",
    "\n",
    "for image in images:\n",
    "    image = Image.open(image)\n",
    "    transformed_image = transform_func(image, (100, 100), crop_pos='center')  # Apply the transformation function\n",
    "    plt.imshow(transformed_image)  # Display the transformed image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72234ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_org = []\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = resize_crop(image)\n",
    "    images_org.append(image)\n",
    "\n",
    "# Create Dataset (transform images, level 1-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771dd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_blurred_center = [transformations.apply_blur_patch(image, crop_size=crop_size, crop_pos='center') for image in images_org]\n",
    "images_blurred_topleft = [transformations.apply_blur_patch(image, crop_size=crop_size, crop_pos='top-left') for image in images_org]\n",
    "images_blurred_topright = [transformations.apply_blur_patch(image, crop_size=crop_size, crop_pos='top-right') for image in images_org]\n",
    "images_blurred_bottomleft = [transformations.apply_blur_patch(image, crop_size=crop_size, crop_pos='bottom-left') for image in images_org]\n",
    "images_blurred_bottomright = [transformations.apply_blur_patch(image, crop_size=crop_size, crop_pos='bottom-right') for image in images_org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_compressed_lvl_1_center = [transformations.apply_compression_patch(image, crop_size=crop_size, quality=2 ,crop_pos='center' ) for image in images_org]\n",
    "images_compressed_lvl_2_center = [transformations.apply_compression_patch(image, crop_size=crop_size, quality=11 ,crop_pos='center' ) for image in images_org]\n",
    "images_compressed_lvl_3_center = [transformations.apply_compression_patch(image, crop_size=crop_size, quality=19 ,crop_pos='center' ) for image in images_org]\n",
    "\n",
    "\n",
    "# plot sample image\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "axes[0].imshow(images_org[img_idx])\n",
    "axes[0].set_title('Original Image')\n",
    "axes[1].imshow(images_compressed_lvl_1_center[img_idx])\n",
    "axes[1].set_title('Compressed lvl 1 Image (Center)')\n",
    "axes[2].imshow(images_compressed_lvl_2_center[img_idx])\n",
    "axes[2].set_title('Compressed lvl 2 Image (Center)')\n",
    "axes[3].imshow(images_compressed_lvl_3_center[img_idx])\n",
    "axes[3].set_title('Compressed lvl 3 Image (Center)')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c89d07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# images compressed center\n",
    "images_compressed_lvl_1_center = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=2 ,crop_pos='center' ) for image in images_org]\n",
    "images_compressed_lvl_2_center = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=11 ,crop_pos='center' ) for image in images_org]\n",
    "images_compressed_lvl_3_center = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=19 ,crop_pos='center' ) for image in images_org]\n",
    "\n",
    "# images compressed topleft\n",
    "images_compressed_lvl_1_topleft = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=2 ,crop_pos='top-left' ) for image in images_org]\n",
    "images_compressed_lvl_2_topleft = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=11 ,crop_pos='top-left' ) for image in images_org]\n",
    "images_compressed_lvl_3_topleft = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=19 ,crop_pos='top-left' ) for image in images_org]\n",
    "\n",
    "# images compressed topright\n",
    "images_compressed_lvl_1_topright = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=2 ,crop_pos='top-right' ) for image in images_org]\n",
    "images_compressed_lvl_2_topright = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=11 ,crop_pos='top-right' ) for image in images_org]\n",
    "images_compressed_lvl_3_topright = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=19 ,crop_pos='top-right' ) for image in images_org]\n",
    "\n",
    "# images compressed bottomleft\n",
    "images_compressed_lvl_1_bottomleft = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=2 ,crop_pos='bottom-left' ) for image in images_org]\n",
    "images_compressed_lvl_2_bottomleft = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=11 ,crop_pos='bottom-left' ) for image in images_org] \n",
    "images_compressed_lvl_3_bottomleft = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=19 ,crop_pos='bottom-left' ) for image in images_org]\n",
    "\n",
    "# images compressed bottomright\n",
    "images_compressed_lvl_1_bottomright = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=2 ,crop_pos='bottom-right' ) for image in images_org]\n",
    "images_compressed_lvl_2_bottomright = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=11 ,crop_pos='bottom-right' ) for image in images_org]\n",
    "images_compressed_lvl_3_bottomright = [transformations.apply_compression_patch(image, crop_size=crop_size,quality=19 ,crop_pos='bottom-right' ) for image in images_org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf6ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot sample image\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 5))\n",
    "axes[0].imshow(images_org[img_idx])\n",
    "axes[0].set_title('Original Image')\n",
    "axes[1].imshow(images_blurred_center[img_idx])\n",
    "axes[1].set_title('Blurred Image (Center)')\n",
    "axes[2].imshow(images_blurred_topleft[img_idx])\n",
    "axes[2].set_title('Blurred Image (Top-Left)')\n",
    "axes[3].imshow(images_blurred_topright[img_idx])\n",
    "axes[3].set_title('Blurred Image (Top-Right)')\n",
    "axes[4].imshow(images_blurred_bottomleft[img_idx])\n",
    "axes[4].set_title('Blurred Image (Bottom-Left)')\n",
    "axes[5].imshow(images_blurred_bottomright[img_idx])\n",
    "axes[5].set_title('Blurred Image (Bottom-Right)')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e64d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot sample image\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 5))\n",
    "axes[0].imshow(images_org[img_idx])\n",
    "axes[0].set_title('Original Image')\n",
    "axes[1].imshow(images_blurred_center[img_idx])\n",
    "axes[1].set_title('Blurred Image (Center)')\n",
    "axes[2].imshow(images_blurred_topleft[img_idx])\n",
    "axes[2].set_title('Blurred Image (Top-Left)')\n",
    "axes[3].imshow(images_blurred_topright[img_idx])\n",
    "axes[3].set_title('Blurred Image (Top-Right)')\n",
    "axes[4].imshow(images_blurred_bottomleft[img_idx])\n",
    "axes[4].set_title('Blurred Image (Bottom-Left)')\n",
    "axes[5].imshow(images_blurred_bottomright[img_idx])\n",
    "axes[5].set_title('Blurred Image (Bottom-Right)')\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_idx, image in enumerate(images_org):\n",
    "\n",
    "    image_tensor = transforms.ToTensor()(images_org[img_idx]).to(device)\n",
    "    image_blur_center_tensor = transforms.ToTensor()(images_blurred_center[img_idx]).to(device)\n",
    "    image_blur_topleft_tensor = transforms.ToTensor()(images_blurred_topleft[img_idx]).to(device)\n",
    "    image_blur_topright_tensor = transforms.ToTensor()(images_blurred_topright[img_idx]).to(device)\n",
    "    image_blur_bottomleft_tensor = transforms.ToTensor()(images_blurred_bottomleft[img_idx]).to(device)\n",
    "    image_blur_bottomright_tensor = transforms.ToTensor()(images_blurred_bottomright[img_idx]).to(device)\n",
    "\n",
    "    image_tensor = transforms.Normalize(mean=mean, std=std)(image_tensor)\n",
    "    image_blur_center_tensor = transforms.Normalize(mean=mean, std=std)(image_blur_center_tensor)\n",
    "    image_blur_topleft_tensor = transforms.Normalize(mean=mean, std=std)(image_blur_topleft_tensor)    \n",
    "    image_blur_topright_tensor = transforms.Normalize(mean=mean, std=std)(image_blur_topright_tensor)\n",
    "    image_blur_bottomleft_tensor = transforms.Normalize(mean=mean, std=std)(image_blur_bottomleft_tensor)\n",
    "    image_blur_bottomright_tensor = transforms.Normalize(mean=mean, std=std)(image_blur_bottomright_tensor)\n",
    "\n",
    "    fig, axes = plt.subplots(len(weight_paths)+1, 11, figsize=(11*3, (len(weight_paths)+1)*2))  # Create subplots\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Plot the original image\n",
    "    axes[0,0].imshow(images_org[img_idx])\n",
    "    axes[0,0].set_title(\"Original Image\")\n",
    "\n",
    "    # Plot the transformed images\n",
    "    axes[0,1].imshow(images_blurred_center[img_idx])\n",
    "    axes[0,1].set_title(\"Blur Center\")\n",
    "\n",
    "    axes[0,2].imshow(images_blurred_topleft[img_idx])\n",
    "    axes[0,2].set_title(\"Blur Top-Left\")\n",
    "\n",
    "    axes[0,3].imshow(images_blurred_topright[img_idx])\n",
    "    axes[0,3].set_title(\"Blur Top-Right\")\n",
    "\n",
    "    axes[0,4].imshow(images_blurred_bottomleft[img_idx])\n",
    "    axes[0,4].set_title(\"Blur Bottom-Left\")\n",
    "\n",
    "    axes[0,5].imshow(images_blurred_bottomright[img_idx])\n",
    "    axes[0,5].set_title(\"Blur Bottom-Right\")\n",
    "\n",
    "    # Plot difference between original and transformed image\n",
    "    axes[0,6].imshow(np.abs(np.array(images_org[img_idx]) - np.array(images_blurred_center[img_idx])))\n",
    "    axes[0,6].set_title(\"Difference Blur Center\")\n",
    "    \n",
    "    axes[0,7].imshow(np.abs(np.array(images_org[img_idx]) - np.array(images_blurred_topleft[img_idx])))\n",
    "    axes[0,7].set_title(\"Difference Blur Top-Left\")\n",
    "\n",
    "    axes[0,8].imshow(np.abs(np.array(images_org[img_idx]) - np.array(images_blurred_topright[img_idx])))\n",
    "    axes[0,8].set_title(\"Difference Blur Top-Right\")\n",
    "\n",
    "    axes[0,9].imshow(np.abs(np.array(images_org[img_idx]) - np.array(images_blurred_bottomleft[img_idx])))\n",
    "    axes[0,9].set_title(\"Difference Blur Bottom-Left\")\n",
    "\n",
    "    axes[0,10].imshow(np.abs(np.array(images_org[img_idx]) - np.array(images_blurred_bottomright[img_idx])))\n",
    "    axes[0,10].set_title(\"Difference Blur Bottom-Right\")\n",
    "\n",
    "\n",
    "    for i, weights_path in enumerate(weight_paths):\n",
    "            \n",
    "        model_number = int(weights_path.split('AIO')[1].split('.pth')[0])\n",
    "        print(f'Model {model_number}: {weights_path}')\n",
    "\n",
    "\n",
    "        # Load the model with the specified weights\n",
    "        model = create_vit_model(weights_path=weights_path)\n",
    "        model.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            outputs = model(image_tensor.unsqueeze(0))\n",
    "            outputs_blur_center = model(image_blur_center_tensor.unsqueeze(0))\n",
    "            outputs_blur_topleft = model(image_blur_topleft_tensor.unsqueeze(0))\n",
    "            outputs_blur_topright = model(image_blur_topright_tensor.unsqueeze(0))\n",
    "            outputs_blur_bottomleft = model(image_blur_bottomleft_tensor.unsqueeze(0))\n",
    "            outputs_blur_bottomright = model(image_blur_bottomright_tensor.unsqueeze(0))\n",
    "\n",
    "            _, attention = get_attention_maps(model, image_tensor, patch_size, device)\n",
    "            _, attention_blur_center = get_attention_maps(model, image_blur_center_tensor, patch_size, device)\n",
    "            _, attention_blur_topleft = get_attention_maps(model, image_blur_topleft_tensor, patch_size, device)\n",
    "            _, attention_blur_topright = get_attention_maps(model, image_blur_topright_tensor, patch_size, device)\n",
    "            _, attention_blur_bottomleft = get_attention_maps(model, image_blur_bottomleft_tensor, patch_size, device)\n",
    "            _, attention_blur_bottomright = get_attention_maps(model, image_blur_bottomright_tensor, patch_size, device)\n",
    "\n",
    "\n",
    "            # print(attention.shape)\n",
    "            n_heads = attention.shape[1]\n",
    "            n_layers = attention.shape[0]\n",
    "\n",
    "            img_gray = preprocess_img_gray(images_org[img_idx])\n",
    "            img_blurred_center_gray = preprocess_img_gray(images_blurred_center[img_idx])\n",
    "            img_blurred_topleft_gray = preprocess_img_gray(images_blurred_topleft[img_idx])\n",
    "            img_blurred_topright_gray = preprocess_img_gray(images_blurred_topright[img_idx])\n",
    "            img_blurred_bottomleft_gray = preprocess_img_gray(images_blurred_bottomleft[img_idx])\n",
    "            img_blurred_bottomright_gray = preprocess_img_gray(images_blurred_bottomright[img_idx])\n",
    "\n",
    "            # Plot the grayscale image with heatmap overlay for Median\n",
    "            layer_attention = attention[5]\n",
    "            layer_blurred_center_attention = attention_blur_center[5]\n",
    "            layer_blurred_topleft_attention = attention_blur_topleft[5]\n",
    "            layer_blurred_topright_attention = attention_blur_topright[5]\n",
    "            layer_blurred_bottomleft_attention = attention_blur_bottomleft[5]\n",
    "            layer_blurred_bottomrigh_attention = attention_blur_bottomright[5]\n",
    "\n",
    "            # print(layer_attention.shape)\n",
    "\n",
    "            layer_mean = np.mean(layer_attention, axis=0)\n",
    "            layer_mean_blurred_center = np.mean(layer_blurred_center_attention, axis=0)\n",
    "            layer_mean_blurred_topleft = np.mean(layer_blurred_topleft_attention, axis=0)\n",
    "            layer_mean_blurred_topright = np.mean(layer_blurred_topright_attention, axis=0)\n",
    "            layer_mean_blurred_bottomleft = np.mean(layer_blurred_bottomleft_attention, axis=0)\n",
    "            layer_mean_blurred_bottomright = np.mean(layer_blurred_bottomrigh_attention, axis=0)\n",
    "\n",
    "            layer_mean_norm = normalize_attention_maps(layer_mean)\n",
    "            layer_mean_norm_blurred_center = normalize_attention_maps(layer_mean_blurred_center)\n",
    "            layer_mean_norm_blurred_topleft = normalize_attention_maps(layer_mean_blurred_topleft)\n",
    "            layer_mean_norm_blurred_topright = normalize_attention_maps(layer_mean_blurred_topright)\n",
    "            layer_mean_norm_blurred_bottomleft = normalize_attention_maps(layer_mean_blurred_bottomleft)\n",
    "            layer_mean_norm_blurred_bottomright = normalize_attention_maps(layer_mean_blurred_bottomright)\n",
    "\n",
    "            heatmap = sns.heatmap(layer_mean_norm, cmap=\"inferno\", alpha=0.7, ax=axes[i+1,0], cbar=False)\n",
    "            axes[i+1,0].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,0].set_title(f\"Att Org Img AIO {model_number}\")\n",
    "\n",
    "            heatmap_blurred_center = sns.heatmap(layer_mean_norm_blurred_center, cmap=\"inferno\", alpha=0.7, ax=axes[i+1,1], cbar=False)\n",
    "            axes[i+1,1].imshow(img_blurred_center_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,1].set_title(f\"Att Blurred Center AIO {model_number}\")\n",
    "\n",
    "            heatmap_blurred_topleft = sns.heatmap(layer_mean_norm_blurred_topleft, cmap=\"inferno\", alpha=0.7, ax=axes[i+1,2], cbar=False)\n",
    "            axes[i+1,2].imshow(img_blurred_topleft_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,2].set_title(f\"Att Blurred Top Left AIO {model_number}\")\n",
    "\n",
    "            heatmap_blurred_topright = sns.heatmap(layer_mean_norm_blurred_topright, cmap=\"inferno\", alpha=0.7, ax=axes[i+1,3], cbar=False)\n",
    "            axes[i+1,3].imshow(img_blurred_topright_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,3].set_title(f\"Att Blurred Top Right AIO {model_number}\")\n",
    "\n",
    "            heatmap_blurred_bottomleft = sns.heatmap(layer_mean_norm_blurred_bottomleft, cmap=\"inferno\", alpha=0.7, ax=axes[i+1,4], cbar=False)\n",
    "            axes[i+1,4].imshow(img_blurred_bottomleft_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,4].set_title(f\"Att Blurred Bottom Left AIO {model_number}\")\n",
    "            \n",
    "            heatmap_blurred_bottomright = sns.heatmap(layer_mean_norm_blurred_bottomright, cmap=\"inferno\", alpha=0.7, ax=axes[i+1,5], cbar=False)\n",
    "            axes[i+1,5].imshow(img_blurred_bottomright_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,5].set_title(f\"Att Blurred Bottom Right AIO {model_number}\")\n",
    "\n",
    "\n",
    "            # Plot the difference between attention and attention_transformed as heatmap with cmap \"seismic\"\n",
    "            diff_blurred_center = layer_mean_norm_blurred_center - layer_mean_norm\n",
    "            diff_blurred_topleft = layer_mean_norm_blurred_topleft - layer_mean_norm\n",
    "            diff_blurred_topright = layer_mean_norm_blurred_topright - layer_mean_norm\n",
    "            diff_blurred_bottomleft = layer_mean_norm_blurred_bottomleft - layer_mean_norm\n",
    "            diff_blurred_bottomright = layer_mean_norm_blurred_bottomright - layer_mean_norm\n",
    "\n",
    "            # round 1 decimal places\n",
    "            diff_blurred_center = np.round(diff_blurred_center, 2)\n",
    "            diff_blurred_topleft = np.round(diff_blurred_topleft, 2)\n",
    "            diff_blurred_topright = np.round(diff_blurred_topright, 2)\n",
    "            diff_blurred_bottomleft = np.round(diff_blurred_bottomleft, 2)\n",
    "            diff_blurred_bottomright = np.round(diff_blurred_bottomright, 2)\n",
    "\n",
    "            # # print largest values in diff\n",
    "            # print(np.sort(diff_blurred_center.flatten())[-1])\n",
    "            # # print 10 smallest values in diff\n",
    "            # print(np.sort(diff_blurred_center.flatten())[0])\n",
    "            # # print sum of diff\n",
    "            # print(np.sum(diff_blurred_center))\n",
    "\n",
    "            heatmap_diff_blurred_center = sns.heatmap(diff_blurred_center, cmap=\"seismic\", alpha=0.7, ax=axes[i+1,6], cbar=False)\n",
    "            axes[i+1,6].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,6].set_title(f\"Diff Blurred Center Img AIO {model_number}\")\n",
    "\n",
    "            heatmap_diff_blurred_topleft = sns.heatmap(diff_blurred_topleft, cmap=\"seismic\", alpha=0.7, ax=axes[i+1,7], cbar=False)\n",
    "            axes[i+1,7].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,7].set_title(f\"Diff Blurred Top Left Img AIO {model_number}\")\n",
    "\n",
    "            heatmap_diff_blurred_topright = sns.heatmap(diff_blurred_topright, cmap=\"seismic\", alpha=0.7, ax=axes[i+1,8], cbar=False)\n",
    "            axes[i+1,8].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,8].set_title(f\"Diff Blurred Top Right Img AIO {model_number}\")\n",
    "\n",
    "            heatmap_diff_blurred_bottomleft = sns.heatmap(diff_blurred_bottomleft, cmap=\"seismic\", alpha=0.7, ax=axes[i+1,9], cbar=False)\n",
    "            axes[i+1,9].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,9].set_title(f\"Diff Blurred Bottom Left Img AIO {model_number}\")\n",
    "\n",
    "            heatmap_diff_blurred_bottomright = sns.heatmap(diff_blurred_bottomright, cmap=\"seismic\", alpha=0.7, ax=axes[i+1,10], cbar=False)\n",
    "            axes[i+1,10].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,10].set_title(f\"Diff Blurred Bottom Right Img AIO {model_number}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Specify the output file path\n",
    "    output_file = f'attention_maps_blur_patches_img_{img_idx}.png'\n",
    "\n",
    "    output_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(output_path)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8422312",
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl = 2\n",
    "crop_pos_str = 'bottomright'\n",
    "\n",
    "images_transformed = images_compressed_lvl_2_bottomright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = f'results/Attention_maps/compressed/{crop_pos_str}/lvl_{str(lvl)}'\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for img_idx, image in enumerate(images_org):\n",
    "\n",
    "    image_tensor = transforms.ToTensor()(images_org[img_idx]).to(device)\n",
    "    image_transformed_tensor = transforms.ToTensor()(images_transformed[img_idx]).to(device)\n",
    "\n",
    "    image_tensor = transforms.Normalize(mean=mean, std=std)(image_tensor)\n",
    "    image_transformed_tensor = transforms.Normalize(mean=mean, std=std)(image_transformed_tensor)\n",
    "\n",
    "    fig, axes = plt.subplots(len(weight_paths)+1, 3, figsize=(10, 30))  # Create subplots\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Plot the original image\n",
    "    axes[0,0].imshow(images_org[img_idx])\n",
    "    axes[0,0].set_title(\"Original Image\")\n",
    "\n",
    "    # Plot the transformed image\n",
    "    axes[0,1].imshow(images_transformed[img_idx])\n",
    "    axes[0,1].set_title(\"Transformed Image\")\n",
    "\n",
    "    # Plot difference between original and transformed image\n",
    "    axes[0,2].imshow(np.abs(np.array(images_org[img_idx]) - np.array(images_transformed[img_idx])))\n",
    "    axes[0,2].set_title(\"Difference\")\n",
    "    \n",
    "\n",
    "    for i, weights_path in enumerate(weight_paths):\n",
    "            \n",
    "        model_number = int(weights_path.split('AIO')[1].split('.pth')[0])\n",
    "        print(f'Model {model_number}: {weights_path}')\n",
    "\n",
    "\n",
    "        # Load the model with the specified weights\n",
    "        model = create_vit_model(weights_path=weights_path)\n",
    "        model.to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            outputs = model(image_tensor.unsqueeze(0))\n",
    "            outputs_transformed = model(image_transformed_tensor.unsqueeze(0))\n",
    "\n",
    "            _, attention = get_attention_maps(model, image_tensor, patch_size, device)\n",
    "            _, attention_transformed = get_attention_maps(model, image_transformed_tensor, patch_size, device)\n",
    "\n",
    "            # print(attention.shape)\n",
    "            n_heads = attention.shape[1]\n",
    "            n_layers = attention.shape[0]\n",
    "\n",
    "            img_gray = preprocess_img_gray(images_org[img_idx])\n",
    "            img_transformed_gray = preprocess_img_gray(images_transformed[img_idx])\n",
    "\n",
    "            # Plot the grayscale image with heatmap overlay for Median\n",
    "            layer_attention = attention[5]\n",
    "            layer_attention_transformed = attention_transformed[5]\n",
    "            # print(layer_attention.shape)\n",
    "\n",
    "            layer_mean = np.mean(layer_attention, axis=0)\n",
    "            layer_mean_transfromed = np.mean(layer_attention_transformed, axis=0)\n",
    "\n",
    "            layer_mean_norm = normalize_attention_maps(layer_mean)\n",
    "            layer_mean_norm_transfromed = normalize_attention_maps(layer_mean_transfromed)\n",
    "\n",
    "            heatmap = sns.heatmap(layer_mean_norm, cmap=\"inferno\", alpha=0.7, ax=axes[i+1,0], cbar=False)\n",
    "            axes[i+1,0].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,0].set_title(f\"Att Org Img AIO {model_number}\")\n",
    "\n",
    "            heatmap_transformed = sns.heatmap(layer_mean_norm_transfromed, cmap=\"inferno\", alpha=0.7, ax=axes[i+1,1], cbar=False)\n",
    "            axes[i+1,1].imshow(img_transformed_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,1].set_title(f\"Att Trans Img AIO {model_number}\")\n",
    "\n",
    "            # Plot the difference between attention and attention_transformed as heatmap with cmap \"seismic\"\n",
    "            diff = layer_mean_norm_transfromed - layer_mean_norm\n",
    "            # round 1 decimal places\n",
    "            diff = np.round(diff, 2)\n",
    "            # print largest values in diff\n",
    "            print(np.sort(diff.flatten())[-1])\n",
    "            # print 10 smallest values in diff\n",
    "            print(np.sort(diff.flatten())[0])\n",
    "            # print sum of diff\n",
    "            print(np.sum(diff))\n",
    "            heatmap_diff = sns.heatmap(diff, cmap=\"seismic\", alpha=0.7, ax=axes[i+1,2], cbar=False)\n",
    "            axes[i+1,2].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i+1,2].set_title(f\"Diff Img AIO {model_number}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Specify the output file path\n",
    "    output_file = f'attention_maps_compressed_patch_{crop_pos_str}_lvl_{str(lvl)}_img_{img_idx}.png'\n",
    "\n",
    "    output_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(output_path)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ab1f2",
   "metadata": {},
   "source": [
    "# Beginn Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5589aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "class TransformedImageDataset(Dataset):\n",
    "    def __init__(self, images, transform_func, levels, crop_size=(50, 50), crop_position=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - images (list): List of image file paths or PIL images.\n",
    "        - transform_func (func): Transformation functions (e.g., apply_compression_patch, apply_blur_patch).\n",
    "        - levels (list): Dictionary mapping each transform function to its list of level parameters.\n",
    "                              Example:\n",
    "                                [{\"quality\": 10}, {\"quality\": 25}, {\"quality\": 50}],\n",
    "                                [{\"blur_radius\": 2}, {\"blur_radius\": 5}, {\"blur_radius\": 8}]\n",
    "        - crop_size (tuple): Size of the cropped area.\n",
    "        - crop_position (str): List of crop positions for transformation (e.g., 'center', 'top-left', etc.).\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.transform_func = transform_func\n",
    "        self.levels = levels\n",
    "        self.crop_size = crop_size\n",
    "        self.crop_position = crop_position or None\n",
    "        \n",
    "        # Calculate total transformations per image\n",
    "        self.transforms_per_image = []\n",
    "        for level in levels:\n",
    "            self.transforms_per_image.append((self.transform_func, level, self.crop_position))\n",
    "        \n",
    "        print(f\"crop positions: {self.crop_position}\")\n",
    "        print(f\"Total levels: {len(levels)}\")\n",
    "        print(f\"Total transformations per image: {len(self.transforms_per_image)}\")\n",
    "        print(f\"Total images: {len(self.images)}\")\n",
    "\n",
    "        # Define the transformation including normalization\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256)])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        img_path = self.images[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\") if isinstance(img_path, str) else img_path\n",
    "        # Preprocess the image\n",
    "        img = self.preprocess(img)\n",
    "        org_img = img.copy()\n",
    "        # Apply all the transformations with the specified crop position and level parameters\n",
    "        transformed_imgs = []\n",
    "        for transform_func, level_params, crop_pos in self.transforms_per_image:\n",
    "            transformed_img = transform_func(img, crop_size=self.crop_size, crop_pos=crop_pos, **level_params)\n",
    "            transformed_imgs.append(transformed_img)\n",
    "        \n",
    "        # Return the original image and an array of the transformed images\n",
    "        return org_img, transformed_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "import random\n",
    "from io import BytesIO\n",
    "\n",
    "# Beispielbilder (ersetze dies durch deine Bildpfade oder PIL-Image-Objekte)\n",
    "images = [\"assets/Test/DSX/202ILSVRC2013_train_00010227.JPEG_I5_Q65.jpeg\", \"assets/Test/DSX/304ILSVRC2013_train_00004689.JPEG_I5_Q67.jpeg\"]  # Füge deine Bildpfade hinzu\n",
    "\n",
    "img_directory_path = 'assets/Test/DSX'\n",
    "\n",
    "# Get the list of image paths from the CSV file\n",
    "# Initialize a list to store the image paths\n",
    "image_paths = []\n",
    "\n",
    "# Read the CSV file and extract the image filenames\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        image_filename = row[0]\n",
    "        vote = row[1]\n",
    "        # fitler only images with vote of 5\n",
    "        if int(vote) == 5:\n",
    "            image_path = os.path.join(img_directory_path, image_filename)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "            \n",
    "# Transformationen und Levels definieren\n",
    "transform_func = transformations.apply_compression_patch\n",
    "levels_compression = [\n",
    "    {\"quality\": 2},\n",
    "    {\"quality\": 11},\n",
    "    {\"quality\": 19}\n",
    "]\n",
    "\n",
    "# transform_func = transformations.apply_blur_patch\n",
    "levels_blur = [\n",
    "    {\"blur_radius\": 6},\n",
    "    {\"blur_radius\": 3.19},\n",
    "    {\"blur_radius\": 2.13}\n",
    "]\n",
    "\n",
    "# Map quality to corresponding blur radius range\n",
    "blur_strengths = {\n",
    "    \"Bad\": random.uniform(3.19, 6.00),\n",
    "    \"Poor\": random.uniform(2.13, 3.19),\n",
    "    \"Fair\": random.uniform(1.32, 2.13)\n",
    "}\n",
    "\n",
    "crop_positions = [\"center\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\"]\n",
    "\n",
    "crop_position = \"top-right\"\n",
    "# get transformation function name from the function name\n",
    "transform_func_name = transform_func.__name__.split(\"_\")[1]\n",
    "crop_size = (112,112)\n",
    "\n",
    "# Dataset initialisieren\n",
    "dataset = TransformedImageDataset(\n",
    "    images=image_paths[:30], # images for testing\n",
    "    transform_func=transform_func,\n",
    "    levels=levels_compression,\n",
    "    crop_size=crop_size,\n",
    "    crop_position=crop_position\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c57b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example single image\n",
    "seed = random.randint(0, len(dataset)-1)\n",
    "org_img, _ = dataset[seed]\n",
    "\n",
    "plt.imshow(org_img)\n",
    "plt.title(f\"Orginal Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example show all levels in 1 row\n",
    "fig, axes = plt.subplots(1, len(levels_compression)+1, figsize=(15, 5))\n",
    "org_img, trans_imgs = dataset[seed]\n",
    "\n",
    "axes[0].imshow(org_img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "\n",
    "for i, img in enumerate(trans_imgs):\n",
    "    axes[i+1].imshow(img)\n",
    "    axes[i+1].set_title(f\"Quality {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba594a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = \"results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL\"\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1,6)]\n",
    "weight_file = weight_files[0]\n",
    "print(weight_file)\n",
    "\n",
    "output_dir = f\"results/Attention_maps/transformations/{transform_func_name}/{crop_position}\"\n",
    "# create output_dir if not exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "layer_idx = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_average_attention_in_crop(attention_map, crop_bbox, patch_size):\n",
    "    \"\"\"\n",
    "    Calculate the average attention within a specified crop region.\n",
    "\n",
    "    Args:\n",
    "    - attention_map (torch.Tensor or np.ndarray): Attention map of shape (H, W).\n",
    "    - crop_bbox (tuple): Bounding box coordinates (left, top, right, bottom) in pixel space.\n",
    "    - patch_size (int): Size of each patch in pixels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Average attention within the cropped region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the bounding box from pixel space to patch space\n",
    "    left, top, right, bottom = [coord // patch_size for coord in crop_bbox]\n",
    "\n",
    "    # Ensure that the attention_map is in numpy format for easier indexing\n",
    "    if isinstance(attention_map, torch.Tensor):\n",
    "        attention_map = attention_map.cpu().numpy()\n",
    "\n",
    "    # Extract the attention map within the crop region\n",
    "    crop_attention = attention_map[top:bottom, left:right]\n",
    "\n",
    "    # Calculate the average attention in the cropped region\n",
    "    avg_attention = np.mean(crop_attention)\n",
    "    \n",
    "    return avg_attention\n",
    "\n",
    "def calculate_sum_attention_in_crop(attention_map, crop_bbox, patch_size):\n",
    "    \"\"\"\n",
    "    Calculate the sum of attention within a specified crop region.\n",
    "\n",
    "    Args:\n",
    "    - attention_map (torch.Tensor or np.ndarray): Attention map of shape (H, W).\n",
    "    - crop_bbox (tuple): Bounding box coordinates (left, top, right, bottom) in pixel space.\n",
    "    - patch_size (int): Size of each patch in pixels.\n",
    "\n",
    "    Returns:\n",
    "    - float: Sum of attention within the cropped region.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the bounding box from pixel space to patch space\n",
    "    left, top, right, bottom = [coord // patch_size for coord in crop_bbox]\n",
    "\n",
    "    # Ensure that the attention_map is in numpy format for easier indexing\n",
    "    if isinstance(attention_map, torch.Tensor):\n",
    "        attention_map = attention_map.cpu().numpy()\n",
    "\n",
    "    # Extract the attention map within the crop region\n",
    "    crop_attention = attention_map[top:bottom, left:right]\n",
    "\n",
    "    # Calculate the sum of attention in the cropped region\n",
    "    sum_attention = np.sum(crop_attention)\n",
    "\n",
    "    return sum_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1938ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_data_avg = []\n",
    "attention_data_sum = []\n",
    "\n",
    "for image_idx in range(len(dataset)):\n",
    "\n",
    "    # Creating a subplot\n",
    "    fig, axes = plt.subplots(len(weight_files)+1, len(levels_compression)*2+1, figsize=((len(levels_compression)*2+1)*5.5, (len(weight_files)+1)*4))\n",
    "\n",
    "    org_img, trans_imgs = dataset[image_idx]\n",
    "    org_img_pre = preprocess_img_gray(org_img)\n",
    "\n",
    "    # Displaying the original image and transformed images in the first row\n",
    "    axes[0, 0].imshow(org_img)\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "    for i, img in enumerate(trans_imgs):\n",
    "        axes[0, i+1].imshow(img)\n",
    "        axes[0, i+1].set_title(f\"Quality {i+1}\")\n",
    "        diff_img = np.abs(np.array(org_img) - np.array(img))\n",
    "        axes[0, i+len(trans_imgs)+1].imshow(diff_img)\n",
    "        axes[0, i+len(trans_imgs)+1].set_title(f\"Difference {i+1}\")\n",
    "\n",
    "    # Displaying the attention maps for each weight_file in the next row\n",
    "    for i, weight_file in enumerate(weight_files):\n",
    "        model = create_vit_model(weights_path=weight_file)\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        weight_name = os.path.basename(weight_file).split('.')[0]\n",
    "\n",
    "        # Calculate attention maps for the original and transformed images\n",
    "        img_pre_org = trans_norm2tensor(org_img, image_size, transformation_function=None)\n",
    "        _, attention_org = get_attention_maps(model, img_pre_org, patch_size, device)\n",
    "        layer_attention_org = attention_org[layer_idx]\n",
    "        layer_mean_org = normalize_attention_maps(np.mean(layer_attention_org, axis=0))\n",
    "\n",
    "        sns.heatmap(layer_mean_org, cmap=\"inferno\", alpha=0.7, ax=axes[i+1, 0])\n",
    "        axes[i+1, 0].imshow(org_img_pre)\n",
    "        axes[i+1, 0].set_title(f\"{weight_name} Original Att Map\")\n",
    "\n",
    "        # Calculate attention maps for the transformed images\n",
    "        imgs_pre_trans = [trans_norm2tensor(img, image_size, transformation_function=None) for img in trans_imgs]\n",
    "        atts_trans = [get_attention_maps(model, img_pre_trans, patch_size, device)[1] for img_pre_trans in imgs_pre_trans]\n",
    "        layer_atts_trans = [att[layer_idx] for att in atts_trans]\n",
    "        layer_mean_trans = [normalize_attention_maps(np.mean(layer_att, axis=0)) for layer_att in layer_atts_trans]\n",
    "\n",
    "        crop_bbox = transformations.get_random_bbox(org_img, crop_size=crop_size, crop_pos=crop_position)\n",
    "        # Store average attention for each transformation level and weight\n",
    "        for j, (lmt, img) in enumerate(zip(layer_mean_trans, trans_imgs)):\n",
    "\n",
    "            # Calculate the sum of attention within the cropped region\n",
    "            sum_attention = calculate_sum_attention_in_crop(\n",
    "                attention_map=lmt, \n",
    "                crop_bbox=crop_bbox, \n",
    "                patch_size=patch_size)\n",
    "\n",
    "            # Store the data in the dictionary\n",
    "            attention_data_sum.append({\"Transformation Level\": f\"Q{j+1}\", \"Attention Sum\": sum_attention, \"Weight File\": weight_name})\n",
    "\n",
    "            avg_attention = calculate_average_attention_in_crop(\n",
    "                attention_map=lmt,\n",
    "                crop_bbox=crop_bbox,  # Bounding box for the cropped area\n",
    "                patch_size=patch_size\n",
    "            )\n",
    "            # Append to attention_data with the transformation level and weight file\n",
    "            attention_data_avg.append({\"Transformation Level\": f\"Q{j+1}\", \"Average Attention\": avg_attention, \"Weight File\": weight_name})\n",
    "\n",
    "            # Plotting attention maps as done previously\n",
    "            sns.heatmap(lmt, cmap=\"inferno\", alpha=0.7, ax=axes[i+1, j+1])\n",
    "            axes[i+1, j+1].imshow(preprocess_img_gray(img))\n",
    "            axes[i+1, j+1].set_title(f\"{weight_name} Q{j+1} Attention Map\")\n",
    "\n",
    "            diff_att = (lmt - layer_mean_org)\n",
    "            diff_att_round = np.round(diff_att, 2)\n",
    "            sns.heatmap(diff_att_round, cmap=\"seismic\", alpha=0.7, ax=axes[i+1, j+len(trans_imgs)+1], vmin=-1, vmax=1)\n",
    "            axes[i+1, j+len(trans_imgs)+1].imshow(preprocess_img_gray(img))\n",
    "            axes[i+1, j+len(trans_imgs)+1].set_title(f\"{weight_name} Diff Q{j+1} Attention Map\")\n",
    "        \n",
    "        # Clear the model from memory\n",
    "        del model\n",
    "\n",
    "    # Adjust layout of subplots\n",
    "    plt.tight_layout()\n",
    "    # Define output path\n",
    "    output_path = os.path.join(output_dir, f\"attention_map_comparison_patch_transformation_{transform_func_name}_img_{image_idx}_layer_{layer_idx}_.png\")\n",
    "    print(f\"Saving image to {output_path}\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Create a DataFrame for boxplot\n",
    "attention_df_sum = pd.DataFrame(attention_data_sum)\n",
    "attention_df_avg = pd.DataFrame(attention_data_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ylim max of all weightfile\n",
    "\n",
    "ylim_max = attention_df_sum[\"Attention Sum\"].max()\n",
    "\n",
    "# Generate a boxplot for each weight file\n",
    "for weight_name in attention_df_sum[\"Weight File\"].unique():\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    sns.boxplot(x=\"Transformation Level\", y=\"Attention Sum\", data=attention_df_sum[attention_df_sum[\"Weight File\"] == weight_name])\n",
    "    plt.ylim(0, ylim_max)\n",
    "    # plt.yticks(np.arange(0, ylim_max + 0.1, 0.1))\n",
    "    plt.xlabel(\"Transformation Level\")\n",
    "    plt.ylabel(\"Attention Sum\")\n",
    "    plt.title(f\"Sum of Attention in Cropped Region for Each Transformation Level ({weight_name})\")\n",
    "\n",
    "    filename = f\"boxplot_{crop_position}_patch_transformation_{transform_func_name}_sum_attention_layer_{layer_idx}_weight_{weight_name}.png\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    print(f\"Saving boxplot to {output_path}\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d296bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a boxplot for each weight file\n",
    "for weight_name in attention_df_avg[\"Weight File\"].unique():\n",
    "    plt.figure(figsize=(8, 12))\n",
    "    sns.boxplot(x=\"Transformation Level\", y=\"Average Attention\", data=attention_df_avg[attention_df_avg[\"Weight File\"] == weight_name])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.xlabel(\"Transformation Level\")\n",
    "    plt.ylabel(\"Average Attention\")\n",
    "    plt.title(f\"Average Attention in Cropped Region for Each Transformation Level ({weight_name})\")\n",
    "\n",
    "    filename = f\"boxplot_{crop_position}_patch_transformation_{transform_func_name}_avg_attention_layer_{layer_idx}_weight_{weight_name}.png\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    print(f\"Saving boxplot to {output_path}\")\n",
    "    plt.savefig(output_path, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaee2cd",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e30dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "import random\n",
    "from io import BytesIO\n",
    "\n",
    "img_directory_path = 'assets/Test/DSX'\n",
    "\n",
    "# Get the list of image paths from the CSV file\n",
    "# Initialize a list to store the image paths\n",
    "image_paths = []\n",
    "\n",
    "# Read the CSV file and extract the image filenames\n",
    "with open(csv_file, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        image_filename = row[0]\n",
    "        vote = row[1]\n",
    "        # fitler only images with vote of 5\n",
    "        if int(vote) == 5:\n",
    "            image_path = os.path.join(img_directory_path, image_filename)\n",
    "            image_paths.append(image_path)\n",
    "\n",
    "            \n",
    "# Transformationen und Levels definieren\n",
    "transform_func = transformations.apply_compression_patch\n",
    "levels_compression = [\n",
    "    {\"quality\": 2},\n",
    "    {\"quality\": 11},\n",
    "    {\"quality\": 19}\n",
    "]\n",
    "\n",
    "# transform_func = transformations.apply_blur_patch\n",
    "levels_blur = [\n",
    "    {\"blur_radius\": 6},\n",
    "    {\"blur_radius\": 3.19},\n",
    "    {\"blur_radius\": 2.13}\n",
    "]\n",
    "\n",
    "# Map quality to corresponding blur radius range\n",
    "blur_strengths = {\n",
    "    \"Bad\": random.uniform(3.19, 6.00),\n",
    "    \"Poor\": random.uniform(2.13, 3.19),\n",
    "    \"Fair\": random.uniform(1.32, 2.13)\n",
    "}\n",
    "\n",
    "crop_positions = [\"center\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\"]\n",
    "\n",
    "crop_position = \"top-right\"\n",
    "# get transformation function name from the function name\n",
    "transform_func_name = transform_func.__name__.split(\"_\")[1]\n",
    "crop_size = (112,112)\n",
    "\n",
    "for crop_position in crop_positions:\n",
    "    # Dataset initialisieren\n",
    "    dataset = TransformedImageDataset(\n",
    "        images=image_paths[:30], # images for testing\n",
    "        transform_func=transform_func,\n",
    "        levels=levels_compression,\n",
    "        crop_size=crop_size,\n",
    "        crop_position=crop_position\n",
    "    )\n",
    "\n",
    "    weights_dir = \"results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL\"\n",
    "    # List of different weight files\n",
    "    weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1,6)]\n",
    "    weight_file = weight_files[0]\n",
    "    print(weight_file)\n",
    "\n",
    "    output_dir = f\"results/Attention_maps/transformations/{transform_func_name}/{crop_position}\"\n",
    "    # create output_dir if not exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    layer_idx = 5\n",
    "\n",
    "    attention_data_avg = []\n",
    "    attention_data_sum = []\n",
    "\n",
    "    for image_idx in range(len(dataset)):\n",
    "\n",
    "        # Creating a subplot\n",
    "        fig, axes = plt.subplots(len(weight_files)+1, len(levels_compression)*2+1, figsize=((len(levels_compression)*2+1)*5.5, (len(weight_files)+1)*4))\n",
    "\n",
    "        org_img, trans_imgs = dataset[image_idx]\n",
    "        org_img_pre = preprocess_img_gray(org_img)\n",
    "\n",
    "        # Displaying the original image and transformed images in the first row\n",
    "        axes[0, 0].imshow(org_img)\n",
    "        axes[0, 0].set_title(\"Original Image\")\n",
    "        for i, img in enumerate(trans_imgs):\n",
    "            axes[0, i+1].imshow(img)\n",
    "            axes[0, i+1].set_title(f\"Quality {i+1}\")\n",
    "            diff_img = np.abs(np.array(org_img) - np.array(img))\n",
    "            axes[0, i+len(trans_imgs)+1].imshow(diff_img)\n",
    "            axes[0, i+len(trans_imgs)+1].set_title(f\"Difference {i+1}\")\n",
    "\n",
    "        # Displaying the attention maps for each weight_file in the next row\n",
    "        for i, weight_file in enumerate(weight_files):\n",
    "            model = create_vit_model(weights_path=weight_file)\n",
    "            model.eval()\n",
    "            model.to(device)\n",
    "            weight_name = os.path.basename(weight_file).split('.')[0]\n",
    "\n",
    "            # Calculate attention maps for the original and transformed images\n",
    "            img_pre_org = trans_norm2tensor(org_img, image_size, transformation_function=None)\n",
    "            _, attention_org = get_attention_maps(model, img_pre_org, patch_size, device)\n",
    "            layer_attention_org = attention_org[layer_idx]\n",
    "            layer_mean_org = normalize_attention_maps(np.mean(layer_attention_org, axis=0))\n",
    "\n",
    "            sns.heatmap(layer_mean_org, cmap=\"inferno\", alpha=0.7, ax=axes[i+1, 0])\n",
    "            axes[i+1, 0].imshow(org_img_pre)\n",
    "            axes[i+1, 0].set_title(f\"{weight_name} Original Att Map\")\n",
    "\n",
    "            # Calculate attention maps for the transformed images\n",
    "            imgs_pre_trans = [trans_norm2tensor(img, image_size, transformation_function=None) for img in trans_imgs]\n",
    "            atts_trans = [get_attention_maps(model, img_pre_trans, patch_size, device)[1] for img_pre_trans in imgs_pre_trans]\n",
    "            layer_atts_trans = [att[layer_idx] for att in atts_trans]\n",
    "            layer_mean_trans = [normalize_attention_maps(np.mean(layer_att, axis=0)) for layer_att in layer_atts_trans]\n",
    "\n",
    "            crop_bbox = transformations.get_random_bbox(org_img, crop_size=crop_size, crop_pos=crop_position)\n",
    "            # Store average attention for each transformation level and weight\n",
    "            for j, (lmt, img) in enumerate(zip(layer_mean_trans, trans_imgs)):\n",
    "\n",
    "                # Calculate the sum of attention within the cropped region\n",
    "                sum_attention = calculate_sum_attention_in_crop(\n",
    "                    attention_map=lmt, \n",
    "                    crop_bbox=crop_bbox, \n",
    "                    patch_size=patch_size)\n",
    "\n",
    "                # Store the data in the dictionary\n",
    "                attention_data_sum.append({\"Transformation Level\": f\"Q{j+1}\", \"Attention Sum\": sum_attention, \"Weight File\": weight_name})\n",
    "\n",
    "                avg_attention = calculate_average_attention_in_crop(\n",
    "                    attention_map=lmt,\n",
    "                    crop_bbox=crop_bbox,  # Bounding box for the cropped area\n",
    "                    patch_size=patch_size\n",
    "                )\n",
    "                # Append to attention_data with the transformation level and weight file\n",
    "                attention_data_avg.append({\"Transformation Level\": f\"Q{j+1}\", \"Average Attention\": avg_attention, \"Weight File\": weight_name})\n",
    "\n",
    "                # Plotting attention maps as done previously\n",
    "                sns.heatmap(lmt, cmap=\"inferno\", alpha=0.7, ax=axes[i+1, j+1])\n",
    "                axes[i+1, j+1].imshow(preprocess_img_gray(img))\n",
    "                axes[i+1, j+1].set_title(f\"{weight_name} Q{j+1} Attention Map\")\n",
    "\n",
    "                diff_att = (lmt - layer_mean_org)\n",
    "                diff_att_round = np.round(diff_att, 2)\n",
    "                sns.heatmap(diff_att_round, cmap=\"seismic\", alpha=0.7, ax=axes[i+1, j+len(trans_imgs)+1], vmin=-1, vmax=1)\n",
    "                axes[i+1, j+len(trans_imgs)+1].imshow(preprocess_img_gray(img))\n",
    "                axes[i+1, j+len(trans_imgs)+1].set_title(f\"{weight_name} Diff Q{j+1} Attention Map\")\n",
    "            \n",
    "            # Clear the model from memory\n",
    "            del model\n",
    "\n",
    "        # Adjust layout of subplots\n",
    "        plt.tight_layout()\n",
    "        # Define output path\n",
    "        output_path = os.path.join(output_dir, f\"attention_map_comparison_patch_transformation_{transform_func_name}_img_{image_idx}_layer_{layer_idx}_.png\")\n",
    "        print(f\"Saving image to {output_path}\")\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Create a DataFrame for boxplot\n",
    "    attention_df_sum = pd.DataFrame(attention_data_sum)\n",
    "    attention_df_avg = pd.DataFrame(attention_data_avg)\n",
    "\n",
    "    # Get ylim max of all weightfile\n",
    "\n",
    "    ylim_max = attention_df_sum[\"Attention Sum\"].max()\n",
    "\n",
    "    # Generate a boxplot for each weight file\n",
    "    for weight_name in attention_df_sum[\"Weight File\"].unique():\n",
    "        plt.figure(figsize=(8, 12))\n",
    "        sns.boxplot(x=\"Transformation Level\", y=\"Attention Sum\", data=attention_df_sum[attention_df_sum[\"Weight File\"] == weight_name])\n",
    "        plt.ylim(0, ylim_max)\n",
    "        # plt.yticks(np.arange(0, ylim_max + 0.1, 0.1))\n",
    "        plt.xlabel(\"Transformation Level\")\n",
    "        plt.ylabel(\"Attention Sum\")\n",
    "        plt.title(f\"Sum of Attention in Cropped Region for Each Transformation Level ({weight_name})\")\n",
    "\n",
    "        filename = f\"boxplot_{crop_position}_patch_transformation_{transform_func_name}_sum_attention_layer_{layer_idx}_weight_{weight_name}.png\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving boxplot to {output_path}\")\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "    # Generate a boxplot for each weight file\n",
    "    for weight_name in attention_df_avg[\"Weight File\"].unique():\n",
    "        plt.figure(figsize=(8, 12))\n",
    "        sns.boxplot(x=\"Transformation Level\", y=\"Average Attention\", data=attention_df_avg[attention_df_avg[\"Weight File\"] == weight_name])\n",
    "        plt.ylim(0, 1)\n",
    "        plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "        plt.xlabel(\"Transformation Level\")\n",
    "        plt.ylabel(\"Average Attention\")\n",
    "        plt.title(f\"Average Attention in Cropped Region for Each Transformation Level ({weight_name})\")\n",
    "\n",
    "        filename = f\"boxplot_{crop_position}_patch_transformation_{transform_func_name}_avg_attention_layer_{layer_idx}_weight_{weight_name}.png\"\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        print(f\"Saving boxplot to {output_path}\")\n",
    "        plt.savefig(output_path, dpi=300)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Funktion zum Visualisieren eines einzelnen Bildes mit seinen Transformationen\n",
    "def visualize_image_and_transformations(dataset, image_idx=0):\n",
    "    # Anzahl der Crop-Positionen und das Originalbild\n",
    "    n_cols = len(dataset.crop_positions) + 1  # +1 für das Originalbild\n",
    "    \n",
    "    # Das erste Bild aus dem Dataset auswählen und alle Transformationen anwenden\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=(15, 5))\n",
    "    \n",
    "    # Originalbild anzeigen\n",
    "    original_img = Image.open(dataset.images[image_idx])\n",
    "    # Define the transformation including normalization\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(256)])\n",
    "    \n",
    "    original_img = preprocess(original_img)\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # Transformierte Bilder für jede Crop-Position anzeigen\n",
    "    for idx, crop_pos in enumerate(dataset.crop_positions, start=1):\n",
    "        # Transformation anwenden (hier nehmen wir die erste Transformation und den ersten Level als Beispiel)\n",
    "        transformed_img, *_ = dataset[image_idx * len(dataset.transforms_per_image) + idx - 1]\n",
    "        \n",
    "        # Bild anzeigen\n",
    "        axes[idx].imshow(transformed_img)\n",
    "        axes[idx].set_title(f\"Crop: {crop_pos}\")\n",
    "        axes[idx].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualisieren des Bildes und seiner Transformationen\n",
    "visualize_image_and_transformations(dataset, image_idx=19)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffca36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_maps(images_org, images_transformed, weight_paths, output_dir, img_idx, label, mean, std, patch_size, device):\n",
    "    \"\"\"\n",
    "    Plots and saves attention maps of original and transformed images for multiple model weights.\n",
    "\n",
    "    Parameters:\n",
    "    - images_org: list of original images\n",
    "    - images_transformed: list of transformed images\n",
    "    - weight_paths: list of paths to model weights\n",
    "    - output_dir: directory to save output images\n",
    "    - img_idx: index of the image to process\n",
    "    - label: label of the transformation applied to the images\n",
    "    - mean: mean for normalization\n",
    "    - std: std for normalization\n",
    "    - patch_size: patch size for attention map\n",
    "    - device: device to perform computation on (e.g., 'cuda' or 'cpu')\n",
    "    \"\"\"\n",
    "    image_tensor = transforms.ToTensor()(images_org[img_idx]).to(device)\n",
    "    image_transformed_tensor = transforms.ToTensor()(images_transformed[img_idx]).to(device)\n",
    "\n",
    "    # Normalization\n",
    "    image_tensor = transforms.Normalize(mean=mean, std=std)(image_tensor)\n",
    "    image_transformed_tensor = transforms.Normalize(mean=mean, std=std)(image_transformed_tensor)\n",
    "\n",
    "    # Set up figure\n",
    "    fig, axes = plt.subplots(len(weight_paths) + 1, 3, figsize=(10, 30))  \n",
    "    for ax in axes.flat:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Plot original, transformed images and difference\n",
    "    axes[0, 0].imshow(images_org[img_idx])\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "    axes[0, 1].imshow(images_transformed[img_idx])\n",
    "    axes[0, 1].set_title(\"Transformed Image\")\n",
    "    axes[0, 2].imshow(np.abs(np.array(images_org[img_idx]) - np.array(images_transformed[img_idx])))\n",
    "    axes[0, 2].set_title(\"Difference\")\n",
    "\n",
    "    for i, weights_path in enumerate(weight_paths):\n",
    "        model_number = int(weights_path.split('AIO')[1].split('.pth')[0])\n",
    "        print(f'Model {model_number}: {weights_path}')\n",
    "\n",
    "        # Load the model with the specified weights\n",
    "        model = create_vit_model(weights_path=weights_path)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            _, attention = get_attention_maps(model, image_tensor, patch_size, device)\n",
    "            _, attention_transformed = get_attention_maps(model, image_transformed_tensor, patch_size, device)\n",
    "\n",
    "            img_gray = preprocess_img_gray(images_org[img_idx])\n",
    "            img_transformed_gray = preprocess_img_gray(images_transformed[img_idx])\n",
    "\n",
    "            # Process the attention maps\n",
    "            layer_attention = attention[5]\n",
    "            layer_attention_transformed = attention_transformed[5]\n",
    "            layer_mean = np.mean(layer_attention, axis=0)\n",
    "            layer_mean_transformed = np.mean(layer_attention_transformed, axis=0)\n",
    "\n",
    "            # Normalize attention maps\n",
    "            layer_mean_norm = normalize_attention_maps(layer_mean)\n",
    "            layer_mean_norm_transformed = normalize_attention_maps(layer_mean_transformed)\n",
    "\n",
    "            # Plot attention maps\n",
    "            sns.heatmap(layer_mean_norm, cmap=\"inferno\", alpha=0.7, ax=axes[i + 1, 0], cbar=False)\n",
    "            axes[i + 1, 0].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i + 1, 0].set_title(f\"Att Org Img AIO {model_number}\")\n",
    "\n",
    "            sns.heatmap(layer_mean_norm_transformed, cmap=\"inferno\", alpha=0.7, ax=axes[i + 1, 1], cbar=False)\n",
    "            axes[i + 1, 1].imshow(img_transformed_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i + 1, 1].set_title(f\"Att Trans Img AIO {model_number}\")\n",
    "\n",
    "            # Calculate and plot the difference\n",
    "            diff = layer_mean_norm_transformed - layer_mean_norm\n",
    "            print(\"Largest difference:\", np.sort(diff.flatten())[-1])\n",
    "            print(\"Smallest difference:\", np.sort(diff.flatten())[0])\n",
    "\n",
    "            sns.heatmap(diff, cmap=\"seismic\", alpha=0.7, ax=axes[i + 1, 2], cbar=False)\n",
    "            axes[i + 1, 2].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "            axes[i + 1, 2].set_title(f\"Diff Img AIO {model_number}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure\n",
    "    output_file = f'attention_maps_{crop_pos}_patch_{label}_img_{img_idx}.png'\n",
    "    output_path = os.path.join(output_dir, output_file)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved attention maps to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85b2248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists\n",
    "all_images_blurred = [\n",
    "    images_blurred_center, \n",
    "    images_blurred_topleft, \n",
    "    images_blurred_topright, \n",
    "    images_blurred_bottomleft, \n",
    "    images_blurred_bottomright\n",
    "]\n",
    "\n",
    "blur_labels = [\"center\", \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of lists\n",
    "images_compressed_levels_center = [\n",
    "    images_compressed_lvl_2_topleft, \n",
    "    images_compressed_lvl_3_topleft\n",
    "]\n",
    "\n",
    "lvl_labels = [\"lvl_2\", \"lvl_3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0acec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_idx, img_trans in enumerate(images_compressed_lvl_1_center):\n",
    "            plot_attention_maps(images_org, images_transformed, weight_paths, output_dir, img_idx, 'Level 1', mean, std, patch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205527b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_dir = 'results/Attention_maps/compressed/topleft'\n",
    "\n",
    "for images_transformed, label in zip(all_images_blurred, blur_labels):\n",
    "    # Create output directory if it doesn't exist based on the transformation type\n",
    "    output_dir = os.path.join(output_base_dir, label)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over each image in the transformed images list\n",
    "    for img_idx in range(0,len(images_transformed)):\n",
    "        plot_attention_maps(images_org, images_transformed, weight_paths, output_dir, img_idx, label, mean, std, patch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec53a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# - add 3 different levels of blur and compression\n",
    "# - calculate the avg attention for all patches inside the transformed area (100x100) of all images\n",
    "# - plot for each weight file a boxplot which shows 3 different levels of blur/compression and their avg attention (x-axis: blur/compression level, y-axis: avg attention)\n",
    "# - plot for each weight file a figure with the original image and their attention map aswell as the transformed image in 3 different levels of blur/compression and their attention map (first row: original image, transformed image, transformed image, transformed image) (rows: atts for each weight for each images) (columns: original image, transformed image, transformed image, transformed image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80426c",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'assets/Test/Obs0.csv'\n",
    "dataset_root =  'assets/Test/DSX'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd353bf",
   "metadata": {},
   "source": [
    "### 2.1 Add Augmentation (Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c172fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the normalization parameters (mean and std)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Define the transformation including normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1222b5e",
   "metadata": {},
   "source": [
    "### 2.2 Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e155d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset loader and test dataset\n",
    "test_dataset = ImageQualityDataset(csv_file,dataset_root, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4427511",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_to_check = 'assets/Test/DSX/45737ILSVRC2014_train_00060591.JPEG_I4_Q31.jpeg'\n",
    "label = test_dataset.get_label_by_image_path(image_path_to_check)\n",
    "print(f\"The label for image '{os.path.basename(image_path_to_check)}' is: {label+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96485153",
   "metadata": {},
   "source": [
    "# 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b72cfc",
   "metadata": {},
   "source": [
    "### Evaluate single weight (AIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85ce21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different weight files\n",
    "weight_file = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO2.pth'\n",
    "results = []\n",
    "example_pred_results = []\n",
    "\n",
    "print(f'Weights-file: {os.path.basename(weight_file)} will be evaluated')\n",
    "# Load the model with different weights\n",
    "model = create_vit_model(weights_path=weight_file)\n",
    "model.eval()\n",
    "\n",
    "# init result lists\n",
    "true_labels = []\n",
    "test_preds = []\n",
    "entropies = []\n",
    "true_entropies =[]\n",
    "weighted_sums = []\n",
    "kl_divs = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for i, (images, image_paths, labels) in enumerate(test_loader, 0):\n",
    "        # images = images.to(device)\n",
    "        # labels = labels.to(device)\n",
    "        print(f\"Example Prediction of Batch: {i}\")\n",
    "        outputs = model(images)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Calculate the true distribution\n",
    "        true_distributions = calculate_label_distributions(labels,device='cpu')\n",
    "        # Get predicition by the maximum probability\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        \n",
    "        formatted_probabilities = [\"{:.4f}\".format(prob) for prob in probabilities[0]]\n",
    "        # print(f\"Predicted Probabilities:{formatted_probabilities}\")\n",
    "        # print(f'Predicted Label: {preds[0]}')\n",
    "\n",
    "\n",
    "        # Calculate Entropy\n",
    "        entropy_values = entropy(probabilities.numpy(),base=np.exp(1), axis=1)\n",
    "        true_entropy_values = entropy(true_distributions.numpy(),base=np.exp(1), axis=1)\n",
    "        # Format entropies in a readble way\n",
    "        entropies.extend(entropy_values)\n",
    "        true_entropies.extend(entropy_values)\n",
    "        # print(f'Mean Entropie of batch: {np.mean(entropy_values)}')\n",
    "\n",
    "        # Calculate KL Divergence\n",
    "        kl_div = torch.nn.functional.kl_div(torch.log(probabilities), true_distributions, reduction='batchmean')\n",
    "        # print(f'KL-Divergence batchmean: {kl_div}')\n",
    "        kl_divs.append(kl_div.item())\n",
    "        \n",
    "        # Define weighting factors\n",
    "        weighting_factors = [0,1,2,3,4]\n",
    "        # Calculate the weighted sum of probabilities\n",
    "        weighted_sum = torch.sum(probabilities * torch.tensor(weighting_factors), dim=1).cpu().numpy()\n",
    "        # Format weighted sum in a readble way\n",
    "        weighted_sums.extend(weighted_sum)\n",
    "        # Example printout for the first batch\n",
    "        if i <=2:\n",
    "            example_pred_result = {\n",
    "                \"Weights File\": os.path.basename(weight_file),\n",
    "                \"Image Name\": os.path.basename(image_paths[i]),\n",
    "                \"True Label\": labels.cpu().numpy()[i],\n",
    "                \"Predicted Label\": preds.cpu().numpy()[i],\n",
    "                \"Weighted Sum of Probability\": weighted_sum[i],\n",
    "                \"True Probability Distribution\": true_distributions[i].cpu().numpy().tolist(),\n",
    "                \"Predicted Probability Distribution\": probabilities[i].cpu().numpy().tolist(),\n",
    "                \"Entropy Value\": entropy_values[i],\n",
    "                \"True Entropy Value\": true_entropy_values[i],\n",
    "                \"KL Divergence (batch-mean)\": kl_divs[i],\n",
    "            }\n",
    "            example_pred_results.append(example_pred_result)\n",
    "        print(f'True-Label: {labels.cpu()[0]}')\n",
    "        print(f'Predicted-Label: {preds.cpu().numpy()[0]}')\n",
    "        print(f'Weighted Sum of Probability: {round(weighted_sum[0],4)}')  # Weighted Sum of Prob\n",
    "        print(f'Predicted Probality Distribution: {[round(prob,4) for prob in probabilities[0].numpy()]}')\n",
    "        print(f'True Probality Distribution: {true_distributions.cpu().numpy()[0]}')\n",
    "        print(f'Entropy Value: {round(entropy_values[0],4)}') # High Value: spreading; Low Value: concentrated\n",
    "        print(f'True Entropy Value: {round(true_entropy_values[0],4)}') # High Value: spreading; Low Value: concentrated\n",
    "        print(f'KL Divergence (batch-mean): {round(kl_div.item(),4)}\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the MSE of weighted sum and ground truth\n",
    "mse_weighted = mean_squared_error(true_labels, weighted_sums)\n",
    "\n",
    "# Calculate the MSE of most likely class and ground truth\n",
    "mse = mean_squared_error(true_labels, test_preds)\n",
    "\n",
    "# Calculate the Mean Entropy\n",
    "mean_entropy = np.mean(entropies)\n",
    "\n",
    "# Calculate the Mean KL Divergence\n",
    "mean_kl_div = np.mean(kl_divs)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(true_labels, test_preds)\n",
    "target_names  = [\"bad\", \"poor\", \"fair\", \"good\", \"excellent\"]\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(true_labels, test_preds, target_names=target_names)\n",
    "\n",
    "# Generate confusion matrix\n",
    "confusion = confusion_matrix(true_labels, test_preds)\n",
    "print('#'*50)\n",
    "print('model summary:')\n",
    "# Save confusion matrix as a figure\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(weight_file.replace(\".pth\", \"_confusion.png\"))\n",
    "plt.close()\n",
    "result = {\n",
    "    \"Weights File\": os.path.basename(weight_file),\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"MSE\": mse,\n",
    "    \"MSE weighted\": mse_weighted,\n",
    "    \"Mean Entropy\": mean_entropy,\n",
    "    \"Mean KL Divergence\": mean_kl_div, \n",
    "    \"Classification Report\": class_report\n",
    "}\n",
    "# Store the results\n",
    "results.append(result)\n",
    "# Print summary\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "print('#'*50)\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_path = weight_file.replace(\".pth\", \"_model_comparison_results.csv\")\n",
    "results_df.to_csv(results_path, index=False)\n",
    "\n",
    "# Save example printouts to a CSV file for this model\n",
    "example_printouts_df = pd.DataFrame(example_pred_results)\n",
    "example_printout_file = weight_file.replace(\".pth\", \"_model_comparison_results_examples.csv\")\n",
    "example_printouts_df.to_csv(example_printout_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f58d8e",
   "metadata": {},
   "source": [
    "### Cross comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f4ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"assets/Test/DSX\"\n",
    "csv_dir = \"assets/Test\"\n",
    "csv_files = [os.path.join(csv_dir, f'Obs{i}.csv') for i in range(1,6)]\n",
    "weight_files = [f'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO{i}.pth'for i in range(6)]\n",
    "\n",
    "print(csv_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4306967",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    # Initialize dataset loader and test dataset\n",
    "    test_dataset = ImageQualityDataset(csv_file,dataset_root, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    print(f\"loaded dataset due to {os.path.basename(csv_file)}\")\n",
    "    example_pred_results = []\n",
    "    aio_acc = []\n",
    "\n",
    "    for weight_file in weight_files:\n",
    "        print(f'Weights-file: {os.path.basename(weight_file)} will be evaluated')\n",
    "        # Load the model with different weights\n",
    "        model.load_state_dict(torch.load(weight_file))\n",
    "        model.eval()\n",
    "        # aio_idx = int(''.join(filter(str.isdigit, os.path.basename(weight_file))))\n",
    "        # init result lists\n",
    "        true_labels = []\n",
    "        test_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images,image_paths, labels) in enumerate(test_loader, 0):\n",
    "                # images = images.to(device)\n",
    "                # labels = labels.to(device)\n",
    "                print(f\"Example Prediction of Batch: {i}\")\n",
    "                outputs = model(images)\n",
    "                true_labels.extend(labels)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "                 \n",
    "                if i != 0:\n",
    "                    true_distributions = calculate_label_distributions(labels,device='cpu')\n",
    "                else:\n",
    "                    true_distributions = torch.nn.functional.one_hot(labels, num_classes).float()\n",
    "\n",
    "                # Get predicition by the maximum probability\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                print(f'Image Name: {os.path.basename(image_paths[i])}')\n",
    "                print(f'True-Label: {labels.cpu()[0]}')\n",
    "                print(f'Predicted-Label: {preds.cpu().numpy()[0]}')\n",
    "                print(f'Predicted Probality Distribution: {[round(prob,4) for prob in probabilities[0].numpy()]}')\n",
    "                print(f'True Probality Distribution: {true_distributions.cpu().numpy()[0]}')\n",
    "\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        accuracy = accuracy_score(true_labels, test_preds)\n",
    "\n",
    "        print('\\n')\n",
    "        print('#'*50)\n",
    "        print('model summary:')\n",
    "        result = {\n",
    "            \"Weights File\": os.path.basename(weight_file),\n",
    "            \"Accuracy (accaptable)\": round(accuracy,2),\n",
    "        }\n",
    "        aio_acc.append(accuracy)\n",
    "        # Print summary\n",
    "        for key, value in result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print('#'*50)\n",
    "        print('\\n')\n",
    "\n",
    "        target_names  = [\"bad\", \"poor\", \"fair\", \"good\", \"excellent\"]\n",
    "        # Generate confusion matrix\n",
    "        confusion = confusion_matrix(true_labels, test_preds)\n",
    "        \n",
    "        # Create a DataFrame from the confusion matrix with target names\n",
    "        # confusion_df = pd.DataFrame(confusion, index=target_names, columns=target_names)\n",
    "\n",
    "        # Ensure the confusion matrix has the correct shape (5x5 in this case)\n",
    "        expected_shape = (len(target_names), len(target_names))\n",
    "\n",
    "        # Create a DataFrame with zeros and the correct shape\n",
    "        confusion_df = pd.DataFrame(0, index=target_names, columns=target_names)\n",
    "\n",
    "        # Update the DataFrame with the values from the provided confusion matrix\n",
    "        confusion_df.loc[confusion_df.index[:len(confusion)], confusion_df.columns[:len(confusion)]] = confusion\n",
    "\n",
    "\n",
    "\n",
    "        obs_name, _ = os.path.splitext(os.path.basename(csv_file))\n",
    "        confusion_path = weight_file.replace(\".pth\", f\"_{obs_name}_confusion.png\")\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(confusion_df, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig(confusion_path)\n",
    "        plt.close()\n",
    "\n",
    "    obs.append(aio_acc)\n",
    "\n",
    "\n",
    "# Create a DataFrame for the obs list with proper headers and index (cross comparison)\n",
    "cc_df = pd.DataFrame(obs, columns=[f\"AIO{i}\" for i in range(6)], index=[f\"Obs{i}\" for i in range(6)])\n",
    "\n",
    "# Specify the path to save the obs table as a CSV file\n",
    "cc_path = os.path.join(os.path.dirname(weight_files[0]),\"cross_comparison.csv\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "cc_df.to_csv(cc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb382491",
   "metadata": {},
   "source": [
    "#### Acceptable Ratio Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"assets/Test\"\n",
    "csv_files = [f'assets/Test/Obs{i}.csv'for i in range(6)]\n",
    "# List of weight files\n",
    "weights_dir = \"results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL\"\n",
    "weight_files = [f'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO{i}.pth'for i in range(6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [[] for _ in range(len(csv_files))]\n",
    "for csv_file in csv_files:\n",
    "    # Initialize dataset loader and test dataset\n",
    "    test_dataset = ImageQualityDataset(csv_file,dataset_root, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    obs_idx = int(''.join(filter(str.isdigit, csv_file)))-1\n",
    "    example_pred_results = []\n",
    "    aio_acc = [0] * len(weight_files)\n",
    "\n",
    "    for weight_file in weight_files:\n",
    "        print(f'Weights-file: {os.path.basename(weight_file)} will be evaluated')\n",
    "        # Load the model with different weights\n",
    "        model.load_state_dict(torch.load(weight_file))\n",
    "        model.eval()\n",
    "        aio_idx = int(''.join(filter(str.isdigit, os.path.basename(weight_file))))-1\n",
    "        # init result lists\n",
    "        true_labels = []\n",
    "        test_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (images,image_paths, labels) in enumerate(test_loader, 0):\n",
    "                # images = images.to(device)\n",
    "                # labels = labels.to(device)\n",
    "                print(f\"Example Prediction of Batch: {i}\")\n",
    "                outputs = model(images)\n",
    "                true_labels.extend(labels)\n",
    "\n",
    "                # Convert logits to probabilities\n",
    "                probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "                 \n",
    "                if obs_idx != 0 and aio_idx !=0 or obs_idx != 0:\n",
    "                    # Calculate the true distribution\n",
    "                    true_distributions = calculate_label_distributions(labels,device='cpu')\n",
    "                else:\n",
    "                    true_distributions = torch.nn.functional.one_hot(labels, num_classes).float()\n",
    "\n",
    "                # Get predicition by the maximum probability\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                print(f'Image Name: {os.path.basename(image_paths[i])}')\n",
    "                print(f'True-Label: {labels.cpu()[0]}')\n",
    "                print(f'Predicted-Label: {preds.cpu().numpy()[0]}')\n",
    "                print(f'Predicted Probality Distribution: {[round(prob,4) for prob in probabilities[0].numpy()]}')\n",
    "                print(f'True Probality Distribution: {true_distributions.cpu().numpy()[0]}')\n",
    "\n",
    "\n",
    "        # Calculate Accuracy\n",
    "        # accuracy = accuracy_score(true_labels, test_preds)\n",
    "        correct_predictions = sum(1 for true, pred in zip(true_labels, test_preds) if true == pred or abs(true - pred) == 1)\n",
    "        total_predictions = len(true_labels)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        print('\\n')\n",
    "        print('#'*50)\n",
    "        print('model summary:')\n",
    "        result = {\n",
    "            \"Weights File\": os.path.basename(weight_file),\n",
    "            \"Accuracy (accaptable)\": round(accuracy,2),\n",
    "        }\n",
    "        aio_acc[aio_idx]=accuracy\n",
    "        # Print summary\n",
    "        for key, value in result.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        print('#'*50)\n",
    "        print('\\n')\n",
    "    obs[obs_idx]=aio_acc\n",
    "\n",
    "\n",
    "# Create a DataFrame for the obs list with proper headers and index\n",
    "obs_df = pd.DataFrame(obs, columns=[f\"AIO{i}\" for i in range(1,6)], index=[f\"Obs{i}\" for i in range(1,6)])\n",
    "\n",
    "# Specify the path to save the obs table as a CSV file\n",
    "obs_path = os.path.join(weights_dir,\"cross_comparison_acceptable_ratio.csv\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "obs_df.to_csv(obs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ba645",
   "metadata": {},
   "source": [
    "# 4. Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b65ab53",
   "metadata": {},
   "source": [
    "### Plot Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e36b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_model()\n",
    "batch_size = 128\n",
    "\n",
    "weights_dir = \"results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL\"\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1,6)]\n",
    "\n",
    "# dir with images\n",
    "images_dir = \"assets/Test/DSX\"\n",
    "\n",
    "# image_paths = get_image_paths_from_dir(images_dir)\n",
    "# image_paths = get_image_paths_from_csv('assets/Test/DSX/global_avg.csv', 5)\n",
    "\n",
    "# select images manually\n",
    "filenames = ['8034ILSVRC2013_train_00034320.JPEG_I4_Q50.jpeg', '10451ILSVRC2013_train_00058547.JPEG_I1_Q2.jpeg', '4692ILSVRC2013_train_00079353.JPEG_I3_Q22.jpeg', '30442ILSVRC2014_train_00005639.JPEG_I1_Q3.jpeg', '4337ILSVRC2013_train_00018603.JPEG_I1_Q8.jpeg']\n",
    "image_paths = [os.path.join(images_dir, filename) for filename in filenames]\n",
    "\n",
    "print(weight_files)\n",
    "print(image_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f341a",
   "metadata": {},
   "source": [
    "#### True Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b00a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_distribution = calculate_label_distributions([1],device=device).cpu().numpy()[0]\n",
    "# Create an index for the x-axis\n",
    "index = range(1, len(true_distribution) + 1)\n",
    "\n",
    "# Plot the probability distribution\n",
    "plt.bar(index, true_distribution, tick_label=index)\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Probabilities')\n",
    "# Add text labels for each bar\n",
    "for i in range(len(true_distribution)):\n",
    "    plt.text(index[i], true_distribution[i], f'{true_distribution[i]:.2f}', ha='center', va='bottom')\n",
    "plt.title('Probability Distribution')\n",
    "plt.savefig('Probability Distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9dcd35",
   "metadata": {},
   "source": [
    "#### Predicted Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec31403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Init Matplotlib-Fig\n",
    "num_weights = len(weight_files)\n",
    "num_images = len(image_paths)\n",
    "fig, axes = plt.subplots(num_images, num_weights+1, figsize=(30, 20))\n",
    "\n",
    "for i in range(len(weight_files)+1):\n",
    "    if i != 0:\n",
    "        model.load_state_dict(torch.load(weight_files[i-1]))\n",
    "\n",
    "    for j, image_path in enumerate(image_paths):\n",
    "        ax = axes[j,i]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if i == 0:  # Only the first column should have images\n",
    "            # Load the image and display it on the y-axis\n",
    "            img = prev_img(image, image_size)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title('Original Image')\n",
    "        else:\n",
    "            img_tensor = trans_norm2tensor(image,image_size)\n",
    "            img = img_tensor.unsqueeze(0).to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                output= model(img)\n",
    "\n",
    "            probabilities = torch.softmax(output, dim=1).cpu().numpy()[0]            # Adjust x-axis values\n",
    "            x = np.arange(1, len(probabilities) + 1)\n",
    "            ax.bar(x, probabilities)\n",
    "            ax.set_title(f'{os.path.splitext(os.path.basename(weight_files[i-1]))[0]}')\n",
    "            # Set Y-scale from 1 to 0\n",
    "            ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c40bbd",
   "metadata": {},
   "source": [
    "#### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c23e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_deviation(probabilities, num_classes=5):\n",
    "    \"\"\"\n",
    "    Calculate the standard deviation of a distribution represented by probabilities.\n",
    "\n",
    "    This function computes the standard deviation of a distribution given a list of probabilities\n",
    "    for each class. It assumes that the probabilities are indexed by class label, starting from 1\n",
    "    up to the specified number of classes.\n",
    "\n",
    "    Parameters:\n",
    "        probabilities (list): A list of probabilities for each class.\n",
    "        num_classes (int): The total number of classes. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        float: The standard deviation of the distribution.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if the number of probabilities matches the number of classes\n",
    "    if len(probabilities) != num_classes:\n",
    "        raise ValueError(\"Number of probabilities and classes should be the same.\")\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    probabilities_tensor = torch.tensor(probabilities, dtype=torch.float32)\n",
    "    values_tensor = torch.arange(1, num_classes + 1)\n",
    "\n",
    "    # Calculate the mean (expectation) of the distribution\n",
    "    mean = torch.sum(probabilities_tensor * values_tensor)\n",
    "\n",
    "    # Calculate the squared difference of each value from the mean\n",
    "    squared_diff = (values_tensor - mean) ** 2\n",
    "\n",
    "    # Calculate the weighted sum of squared differences\n",
    "    weighted_sum = torch.sum(probabilities_tensor * squared_diff)\n",
    "\n",
    "    # Calculate the standard deviation as the square root of the weighted sum\n",
    "    std_deviation = torch.sqrt(weighted_sum)\n",
    "\n",
    "    return std_deviation.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022ad4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_probabilities(label, predicted_probabilities, ax):\n",
    "    \"\"\"\n",
    "    Compare predicted probabilities with true distribution for a given label.\n",
    "\n",
    "    This function visualizes the predicted probabilities and the true distribution\n",
    "    for a given label using a bar chart. The predicted probabilities are plotted\n",
    "    in orange, while the true distribution is plotted in black.\n",
    "\n",
    "    Parameters:\n",
    "        label (int): The true label for comparison.\n",
    "        predicted_probabilities (list or numpy.ndarray): Predicted probabilities for each class.\n",
    "        ax (matplotlib.axes.Axes): The matplotlib axes object for plotting.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Calculate the true distribution for the given label\n",
    "    true_distribution = calculate_label_distributions(label, device=device)\n",
    "    true_distribution = true_distribution.cpu().numpy()[0]\n",
    "\n",
    "    # Set the offset for better visualization\n",
    "    offset = 0.35\n",
    "\n",
    "    # Plot the predicted probabilities and true distribution as bar charts\n",
    "    ax.bar(np.arange(len(predicted_probabilities)), predicted_probabilities, width=0.35, label='predicted', align='center', color='orange')\n",
    "    ax.bar(np.arange(len(true_distribution)) + offset, true_distribution, width=0.35, label='true', align='center', color='black')\n",
    "\n",
    "    # Set plot limits, labels, and ticks\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.set_xlabel('Class')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_xticks(np.arange(len(true_distribution)) + offset/2)\n",
    "    ax.set_xticklabels(np.arange(len(true_distribution)))\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = 'assets/Test'\n",
    "csv_files = [os.path.join(csv_dir, f'Obs{i}.csv') for i in range(1,6)]\n",
    "dataset_root = 'assets/Test/DSX'\n",
    "num_weights = len(weight_files)\n",
    "num_images = len(image_paths)\n",
    "fig, axes = plt.subplots(num_images, num_weights+1, figsize=(4.5 * num_weights+1, 3 * num_images))\n",
    "\n",
    "for i in range(len(weight_files)+1):\n",
    "    if i != 0:\n",
    "        model.load_state_dict(torch.load(weight_files[i-1]))\n",
    "        dataset = ImageQualityDataset(csv_files[i-1], dataset_root, transform=transform)\n",
    "    for j, image_path in enumerate(image_paths):\n",
    "        ax = axes[j,i]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if i == 0:  # Only the first column should have images\n",
    "            # Load the image and display it on the y-axis\n",
    "            img = prev_img(image, image_size)\n",
    "            ax.imshow(img)\n",
    "            ax.set_title('Original Image')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            img_tensor = trans_norm2tensor(image,image_size)\n",
    "            img = img_tensor.unsqueeze(0).to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                output= model(img)\n",
    "\n",
    "            label = dataset.get_label_by_image_path(os.path.join(dataset_root, os.path.basename(image_path)))\n",
    "            label_tensor = torch.tensor(label).unsqueeze(0).cpu().numpy()\n",
    "\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            std_deviation_result = standard_deviation(probabilities[0].cpu().numpy())\n",
    "            print(f\"Standard deviation for image {j+1} and AIO {i}: {std_deviation_result}\")\n",
    "            compare_probabilities(label_tensor, probabilities[0].cpu().numpy(), ax)\n",
    "            ax.set_title(f'{os.path.splitext(os.path.basename(weight_files[i-1]))[0]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a818b2",
   "metadata": {},
   "source": [
    "### Plot Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451da25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "depth = 6\n",
    "num_classes = 5\n",
    "patch_size = 16\n",
    "\n",
    "model = create_vit_model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b7327",
   "metadata": {},
   "source": [
    "#### Plot Attention Map of Single AIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b3da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc import transformations\n",
    "\n",
    "blur_func = transformations.apply_blur_patch\n",
    "crop_func = transformations.apply_random_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe976c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO5.pth'\n",
    "image_path = 'assets/Test/DSX/625ILSVRC2013_train_00003983.JPEG_I5_Q95.jpeg'\n",
    "layer_idx = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Hilfsfunktion zum Auswählen eines zufälligen Rechtecks im Bild\n",
    "def get_random_bbox(img, crop_size=(50, 50)):\n",
    "    \"\"\"Gibt ein zufälliges Rechteck im Bild zurück.\"\"\"\n",
    "    width, height = img.size\n",
    "    crop_width, crop_height = crop_size\n",
    "    left = random.randint(0, width - crop_width)\n",
    "    top = random.randint(0, height - crop_height)\n",
    "    right = left + crop_width\n",
    "    bottom = top + crop_height\n",
    "    return (left, top, right, bottom)\n",
    "\n",
    "# Transformationen für Teilbereiche definieren\n",
    "def apply_random_crop(img, crop_size=(50, 50)):\n",
    "    \"\"\"Zufälligen Bildausschnitt entfernen und durch Schwarz ersetzen.\"\"\"\n",
    "    img = img.copy()\n",
    "    bbox = get_random_bbox(img, crop_size)\n",
    "    # Ersetze den Bereich durch eine schwarze Fläche\n",
    "    img.paste((0, 0, 0), bbox)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b1378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = create_vit_model(weights_path=weight_file)\n",
    "model.eval()\n",
    "image = Image.open(image_path)\n",
    "image = prev_img(image, image_size)\n",
    "# image = crop_func(image, crop_size=(10,10))\n",
    "image = blur_func(image)\n",
    "img_pre = trans_norm2tensor(image, image_size, transformation_function=None)\n",
    "_, attention = get_attention_maps(model, img_pre, patch_size, device)\n",
    "print(attention.shape)\n",
    "n_heads = attention.shape[1]\n",
    "n_layers = attention.shape[0]\n",
    "\n",
    "img_pre = prev_img(image, image_size, transformation_function=None)\n",
    "img_gray = prev_img_gray(image, image_size, transformation_function=None)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 5))  # Create subplots\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Plot the original image\n",
    "axes[0].imshow(img_pre)\n",
    "axes[0].set_title(\"Original Image\", fontsize=20, fontname='cmr10')\n",
    "\n",
    "# Plot the grayscale image with heatmap overlay for Median\n",
    "layer_attention = attention[layer_idx]\n",
    "print(layer_attention.shape)\n",
    "\n",
    "layer_mean = np.mean(layer_attention, axis=0)\n",
    "layer_mean_norm = normalize_attention_maps(layer_mean)\n",
    "\n",
    "heatmap = sns.heatmap(layer_mean_norm, cmap=\"inferno\", alpha=0.7, ax=axes[1])\n",
    "axes[1].imshow(img_gray, cmap='gray', alpha=0.5)\n",
    "axes[1].set_title(\"Attention\", fontsize=20, fontname='cmr10')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8019fc",
   "metadata": {},
   "source": [
    "#### Plot Attention across all layers and all heads (no comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064041b",
   "metadata": {},
   "source": [
    "Single AIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e72144",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL/AIO0.pth'\n",
    "image_path = 'assets/manual_testing_imgs/dark/596ILSVRC2013_train_00009848.JPEG_I5_Q95.jpeg'\n",
    "\n",
    "model = create_vit_model(weights_path=weight_file)\n",
    "model.eval()\n",
    "image = Image.open(image_path)\n",
    "image = prev_img(image, image_size)\n",
    "image = transformations.apply_blur_patch(image)\n",
    "visualize_all_layer_head_attention_maps(model, image, image_size, patch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac572069",
   "metadata": {},
   "source": [
    "Multiple AIOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac68bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'assets/manual_testing_imgs/focus/637ILSVRC2013_train_00009458.JPEG_I4_Q37.jpeg'\n",
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(0, 6)]\n",
    "\n",
    "image = Image.open(image_path)\n",
    "image = prev_img(image, image_size)\n",
    "image = transformations.apply_blur_patch(image)\n",
    "\n",
    "\n",
    "for weight_file in weight_files:\n",
    "    model = create_vit_model(weights_path=weight_file)\n",
    "    model.eval()\n",
    "    visualize_all_layer_head_attention_maps(model, image, image_size, patch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c4057",
   "metadata": {},
   "source": [
    "#### Plot Attention across specific layer (comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(6)]\n",
    "\n",
    "images_dir = 'assets/work_imgs/fg_bg'\n",
    "image_paths = get_image_paths_from_dir(images_dir)\n",
    "\n",
    "output_dir = 'results/Attention_maps/comparisons'\n",
    "\n",
    "layer_idx = -1\n",
    "\n",
    "plot_attention_maps_comparison(weight_files,image_paths, image_size, patch_size,output_dir, layer_idx,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3732680",
   "metadata": {},
   "source": [
    "#### Plot mean head of all layers for every AIO (compare layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7046662",
   "metadata": {},
   "source": [
    "No sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a88989",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(6)]\n",
    "images_dir = 'assets/manual_testing_imgs/blurry'\n",
    "image_paths = get_image_paths_from_dir(images_dir)\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image = Image.open(image_path)\n",
    "    image = prev_img(image, image_size)\n",
    "    image = transformations.apply_blur_patch(image)\n",
    "    get_attention_maps_across_weights(model, image, image_size, patch_size, depth, weight_files, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8b7d3",
   "metadata": {},
   "source": [
    "Sub avg (in comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c7a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1,6)]\n",
    "images_dir = 'assets/manual_testing_imgs/focus'\n",
    "num_files = 5\n",
    "image_paths = get_image_paths_from_dir(images_dir,num_files)\n",
    "depth = 6\n",
    "\n",
    "csv_file_path = 'assets/Test/DSX/global_avg.csv'\n",
    "# image_paths = get_image_paths_from_csv(csv_file_path, num_files)\n",
    "for image_path in image_paths:\n",
    "    img = Image.open(image_path)\n",
    "    # Preprocess the image\n",
    "    img = prev_img(img, image_size)\n",
    "    img = transformations.apply_blur_patch(img, crop_size=(100,100))\n",
    "    get_attention_maps_with_deviation(img, weight_files, image_size,depth, patch_size, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d709fe4",
   "metadata": {},
   "source": [
    "#### Plot Attention of Images sorted by global avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5710f6",
   "metadata": {},
   "source": [
    "Load Dataset to get global average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "images_dir = 'assets/Test/DSX'\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1, 6)]\n",
    "layer_idx = -1\n",
    "\n",
    "# Create a DataLoader\n",
    "global_avg_dataset = ImageAttentionGlobalAvgDataset(images_dir, weight_files, image_size, patch_size, layer_idx, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f89210",
   "metadata": {},
   "source": [
    "Write for each image the global avg and path to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = os.path.join(images_dir, 'global_avg.csv')\n",
    "\n",
    "global_avg_dataset.write_to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74723bea",
   "metadata": {},
   "source": [
    "Get the top 5 images with the highest difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43624ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"test: {global_avg_dataset[0]}\")\n",
    "top_5 = global_avg_dataset.get_top_global_avg(5,True)\n",
    "# print(f\"Global Avg: {top_5}\")\n",
    "for i in range(len(top_5)):\n",
    "    fieldnames = ['filename', 'global_avg', 'org_img', 'att_maps']\n",
    "    selected_fields = {key: top_5[i][key] for key in fieldnames}\n",
    "    print(selected_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df080a",
   "metadata": {},
   "source": [
    "Get the top 5 images with the lowest difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_neg = global_avg_dataset.get_top_global_avg(5,False)\n",
    "\n",
    "for i in range(len(top_5_neg)):\n",
    "    fieldnames = ['filename', 'global_avg', 'org_img', 'att_maps']\n",
    "    selected_fields = {key: top_5_neg[i][key] for key in fieldnames}\n",
    "    print(selected_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d6dd8",
   "metadata": {},
   "source": [
    "Plot attention of images with highest global avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55fbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'results/Attention_maps/transformations/grayscale'\n",
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "images_dir = 'assets/Test/DSX'\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1, 6)]\n",
    "layer_idx = 5\n",
    "\n",
    "# Init figure with axes len(weight_files) x len(top_5)\n",
    "fig, axes = plt.subplots(nrows=len(top_5)+1, ncols=len(weight_files), figsize=(15, 10))\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "for i in range(len(top_5)):\n",
    "    filename = top_5[i]['filename']\n",
    "    img = top_5[i]['org_img']\n",
    "    global_avg = top_5[i]['global_avg']\n",
    "    att_maps = top_5[i]['att_maps']\n",
    "\n",
    "    # Plot original image\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f'Global Avg: {global_avg:.4f}')\n",
    "    axes[0, i].axis('off')\n",
    "    img_gray = prev_img_gray(img, image_size)\n",
    "    # Plot attention maps for each weight file\n",
    "    for j in range(len(weight_files)):\n",
    "        weight_file = weight_files[j]\n",
    "        att_map = att_maps[j]\n",
    "        ax = axes[j+1, i]\n",
    "        ax.imshow(img_gray)\n",
    "        sns.heatmap(att_map, cmap=\"inferno\", alpha=0.6, ax=ax, vmin=0, vmax=1)\n",
    "        ax.set_title(f'{os.path.splitext(os.path.basename(weight_file))[0]} Layer {layer_idx + 1} Attention')\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'attention_maps_highest_differences.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b9507d",
   "metadata": {},
   "source": [
    "Plot attention of images with lowest global avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da8f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'results/Attention_maps/transformations/grayscale'\n",
    "weights_dir = 'results/weights/Cross-Entropy_3_Iter_var_0.4/FINAL'\n",
    "images_dir = 'assets/Test/DSX'\n",
    "# List of different weight files\n",
    "weight_files = [os.path.join(weights_dir, f'AIO{i}.pth') for i in range(1, 6)]\n",
    "layer_idx = 5\n",
    "\n",
    "# Init figure with axes len(weight_files) x len(top_5)\n",
    "fig, axes = plt.subplots(nrows=len(weight_files)+1, ncols=len(top_5_neg), figsize=(15, 10))\n",
    "for ax in axes.flat:\n",
    "    ax.axis('off')\n",
    "\n",
    "print(len(top_5_neg))\n",
    "for i in range(len(top_5_neg)):\n",
    "    filename = top_5_neg[i]['filename']\n",
    "    img = top_5_neg[i]['org_img']\n",
    "    global_avg = top_5_neg[i]['global_avg']\n",
    "    att_maps = top_5_neg[i]['att_maps']\n",
    "\n",
    "    # Plot original image\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f'Global Avg: {global_avg:.4f}')\n",
    "    axes[0, i].axis('off')\n",
    "    img_gray = prev_img_gray(img, image_size)\n",
    "    # Plot attention maps for each weight file\n",
    "    for j in range(len(weight_files)):\n",
    "        weight_file = weight_files[j]\n",
    "        att_map = att_maps[j]\n",
    "        ax = axes[j+1, i]\n",
    "        ax.imshow(img_gray)\n",
    "        sns.heatmap(att_map, cmap=\"inferno\", alpha=0.6, ax=ax, vmin=0, vmax=1)\n",
    "        ax.set_title(f'{os.path.splitext(os.path.basename(weight_file))[0]} Layer {layer_idx + 1} Attention')\n",
    "\n",
    "# Save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'attention_maps_lowest_differences.png'))\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit-iqa-python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
